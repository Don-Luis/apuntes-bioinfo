%20/02 - Ana González Marcos
\chapter{Aplicación de algoritmos a problemas bioinformáticos}
La búsqueda de motivos se realiza mediante búsqueda exhaustiva, algoritmos codiciosos o algoritmos aleatorios. Los algoritmos de programación dinámica se emplean para los alineamientos. Un motivo es una secuencia específica y corta que aparecen frecuentemente en una región del ADN o secuencia proteica. Representa una región conservada en secuencias que suelen tener significado biológico. 

\section{Búsqueda exhaustiva}
La búsqueda exhaustiva es la única búsqueda "de verdad" que da una solución determinista exacta. También se conoce como búsqueda por fuerza bruta. El algoritmo examina todas las posibles alternativas para encontrar una solución óptima. El tiempo de cómputo es muy alto, pese a que requiera de poco esfuerzo en su diseño. Como las secuencias biológicas son muy largas, este tipo de búsquedas es inviable. 
En el patrón se busca encontrar un consenso que se puede representar gráficamente en logos. 

Una posible aplicación es la búsqueda de un patrón de x nucleótidos (x-mer) sin mutaciones. También se pueden incorporar un par de mutaciones y generar un consenso. Con esto se calcula el perfil y se ve cuántos nucleótidos caen en cada posición. Se busca el perfil que, aun con mutaciones, da un motivo consenso. 

El consenso es un motivo ancestro del cual surgen los motivos mutados. Para calcular la medida que indique lo buen consenso que es un motivo, se tiene en cuenta la homología, es decir, la similitud debida a un ancestro común. La distancia entre un motivo real y la secuencia de consenso suele ser menor que la de dos motivos reales. Necesitamos introducir una función de puntuación para comparar diferentes conjeturas (consenso) y elegir la «mejor». 
$$Score(s, DNA) = \sum^l_{i = 1} \max_{k \in A T G C} count(k, i)$$

Se busca a lo largo de todas las cadenas, desde la primera posición el tamaño del l-mer. En definitiva, el algoritmo es:
\begin{lstlisting}
BruteForceMotifSearch(DNA, t, n, l)
bestScore = 0
for each s=(s1, s2, ..., st) from (1, 1 ... 1) to (n-l + 1, ..., n-l + 1)
	if Score(s, DNA) > bestScore
		bestScore = score(s, DNA)
		bestMotif = (s1, s2, ... st)
return bestMotif
\end{lstlisting}

Como esta búsqueda es exhaustiva, para $(n - l + 1)$ posiciones en t secuencias, se buscan $(n - l + 1)^t$ sets de posiciones iniciales. Para cada posición inicial, la función de puntuación hace $l$ operaciones, de forma que la complejidad es:
$$l \cdot (n - l + 1)^t \rightarrow O(l \cdot n^t)$$
De esta forma, hay veces en las que no es posible hacer este cálculo.

\subsection{Median String}
El median string es la alternativa a la búsqueda exhaustiva. En este caso se busca en las secuencias un patrón. No se utiliza la puntuación de antes, si no que utiliza la distancia de Hamming. La distancia de Hamming entre dos mer se define como el número de nucleótidos distintos entre ellos. 
$$d_H(AAAAAA, ACAAAC) = 2$$
$$d_H(v, s) = \sum^t_{i = 1} d_H(v, s_i)$$
Antes se buscaba la puntuación máxima, pero ahora buscamos la distancia mínima. 

\begin{lstlisting}
MedianStringSearch(DNA, t, n, l)
bestWord = AAAAA...A
bestDistance = infinity
for each l-mer v from AAA...A to TTT...T
	if TotalDistance(v, DNA) < bestDistance:
		bestDistance = TotalDistance(v, DNA)
		bestWord = v
return bestWord
\end{lstlisting}

La búsqueda de motivos es un problema de maximización, mientras que la cadena mediana es un problema de minimización. Sin embargo, el problema de la Búsqueda de Motivos y el de la Cadena Mediana son computacionalmente equivalentes. Hay que demostrar que minimizar TotalDistance es equivalente a maximizar la puntuación.

Con este algoritmo, la complejidad es de $4^l$. El coste es considerablemente más bajo que en la búsqueda exhaustiva, ya que el tamaño del mer es más pequeño que el tamaño de la secuencia. Por tanto, reformular un problema puede ayudar a disminuir la complejidad computacional. 

%25/02 - Ana
\section{Algoritmo codicioso}
Un algoritmo codicioso es un algoritmo que siempre toma la mejor solución inmediata, o local, mientras encuentra una respuesta.
Los algoritmos codiciosos:
\begin{itemize}
\item a menudo devuelven resultados subóptimos, pero tardan poco en hacerlo.
\item seleccionan la alternativa «más atractiva» en cada iteración.
\item suelen ser heurísticas rápidas que cambian precisión por velocidad para encontrar una solución aproximada/subóptima.
\end{itemize}

En este caso, se utiliza la entropía como medida. Es una medida probabilística que se suele definir como $\int -p \log (p)$. Está la entropía sobre todos los nucleótidos se correspondería a $- \sum p \cdot \log_2(p)$. Esto se utiliza para calcular los logos. Motif logo es un diagrama para visualizar la conservación de motivos que consiste en una pila de letras en cada posición. La altura total de cada columna se basa en el contenido informativo de la columna, que se define como $2 - H_i(p_A, p_C, p_G, p_T)$. Cuanto menor es la entropía, mayor es el contenido de información, lo que significa que las columnas altas del logotipo del motivo están muy conservadas. 

La probabilidad del l-mer es la probabilidad de que un l-mer a fuese creado por el perfil P:
$$p(a|P) = \Pi^l_{k=1} P_{a_{i,k}}$$ 
donde $P_{a_{i,k}}$ es la probabilidad de la letra $a_i$ en la posición $k$, distribuida independientemente e idénticamente. 

\begin{lstlisting}
GreedyMotifSearch(DNA, k, t)
BestMotifs = primer k-mer de cada string de ADN
for each k-mer Motif in the first string in DNA do
	Motif1 = Motif
	for i = 2 to t do
		form Profile from Motif1, ..., Motif i-1
		Motif i = profile-most probable k-mer in the ith string in DNA
	Motifs = Motif i, ..., Motif t
	if Score(Motif) > Score (BestMotif) then
		BestMotifs = Motifs
return BestMotifs
\end{lstlisting}

Primero, se inicializa una matriz de motivos con los primeros -lmers de cada secuencia de ADN. Después se va comparando el l-mer de la primera secuencia con la siguiente secuencia, calculando el mejor motivo, el cual se añade a una nueva lista. Así se recorren las distintas secuencias. Tras obtener la lista de los motivos (uno por cada secuencia), se obtiene el consenso. Éste se puntúa y se compara con la puntuación de la matriz inicializada, quedándonos con la matriz que tenga una puntuación mayor. A continuación se mueve el primer l-mer un nucleótido y repetir todo este proceso.

Para evitar obtener resultados de 0, se utiliza la regla de sucesión de Laplace, sumando 1 a todos los valores de contaje para computar las probabilidades mediante pseudo-counts. De esta forma, se obtiene la matriz del perfil. 

En cuanto al tiempo del cómputo, como tenemos una matriz $t \times n$ de ADN y una longitud del patrón $l$, el tiempo de ejecución es $O(n^2 \cdot l \cdot t)$, lo cual es mejor que el algoritmo de fuerza bruta. No obstante, es restrictivo, por lo que podemos no encontrar los mejores motivos. Cambiando el orden de las secuencias, el resultado va a ser distinto, afectando significativamente el rumbo que tomará el análisis. 

\section{Algoritmos randomizados}
Los algoritmos aleatorios toman decisiones aleatorias en lugar de deterministas. Estos algoritmos se utilizan en situaciones en las que no se conoce ningún algoritmo polinómico correcto. Seleccionan aleatoriamente posibles ubicaciones y encuentran una forma de cambiar codiciosamente esas ubicaciones hasta que hayamos convergido al motivo oculto.

\begin{lstlisting}
RandomizedMotifSearch(DNA, l, t)
Randomly select l-mers Motifs = (Motif1, ... Motift) in each string from DNA
BestMotifs = Motifs
while forever
	Profile = PROFILE(Motifs)
	Motifs = MOTIF(Profile, DNA)
	if Score(Motifs) > Score(BestMotifs)
		BestMotifs = Motifs
	else
		return BestMotifs
\end{lstlisting}

Este algoritmo se detiene tras un número de iteraciones que le demos, o cuando el perfil apenas se modifique tras más iteraciones. El algoritmo se ejecuta muchas veces, cada una de ellas con nuevos l-mers inicializados, y con la colección de los motivos con mayor puntuación se obtiene el consenso. 

\subsection{Gibbs Sampling}
RandomizedMotifSearch puede cambiar todas las cadenas $t$ en Motifs en una única iteración. Puede sondear imprudentemente, y algunos motivos correctos pueden ser descartados en la siguiente iteración.

GibbsSampler es un algoritmo iterativo más cauteloso que descarta un único $l$-mers del conjunto actual de motivos en cada iteración y decide conservarlo o sustituirlo por uno nuevo. Los pasos son:
\begin{enumerate}
\item Elegir aleatoriamente las posiciones iniciales de l-mers para cada secuencia.
\item Elegir al azar una de las secuencias $t$
\item Crear un perfil $P$ (consenso) a partir de las otras secuencias ($t - 1$)
\item Para cada posición de la secuencia eliminada, se calcula la probabilidad de que el $l$-mer que comienza en esa posición haya sido generado por $P$.
\item Elegir al azar una nueva posición inicial para la secuencia eliminada basándose en las probabilidades calculadas en el paso anterior.
\end{enumerate}

Este proceso se itera hasta dar con la solución, es decir, que la puntuación no mejore más. 
GibbsSampler funciona bien en muchos casos. Dado que GibbsSampler explora sólo un pequeño subconjunto de soluciones, puede «atascarse» en un óptimo local. Al igual que RandomizedMotifSearch, debe ejecutarse varias veces con la esperanza de que una de estas ejecuciones produzca los motivos con mejor puntuación.

Estos algoritmos aleatorizados funcionan porque el ADN no es del todo aleatorio, conteniendo motivos reguladores que permiten un control preciso sobre la expresión genética. Estos motivos resultan en un perfil esperado sesgado.

%27/02
\section{Algoritmo divide-y-vencerás}
En estos algoritmos, el problema se divide en subproblemas, conquistándolos de forma recursiva. Si los subproblemas son lo suficientemente pequeños, se resuelven de forma bruta. Las soluciones de los subproblemas se deben combinar en una solución para el problema original, siendo esto lo complicado. Un algoritmo de divide y vencerás hace más trabajo del necesario, resolviendo repetidamente los subproblemas comunes. 

\section{Programación dinámica}
La programación dinámica se basa en la premisa de calcular primero las soluciones a subproblemas más pequeños y utilizarlas después para resolver problemas sucesivamente más grandes hasta obtener la respuesta. La programación dinámica se utiliza cuando se podría recurrir a la recursividad, pero sería ineficaz porque resolvería repetidamente los mismos subproblemas. Se toma un problema que podría resolverse recursivamente de arriba abajo y, en su lugar, se resuelve iterativamente de abajo arriba. Los resultados intermedios se almacenan en una tabla para su uso posterior; de lo contrario, se acabarían calculando repetidamente, lo que constituye un algoritmo ineficaz.

Por ejemplo, la programación dinámica se utiliza para ver la similitud entre genes. El alineamiento de secuencias es esencial para filogenia, análisis genómico, predicción de genes, estructura proteica, estructura de ARN secundario, búsqueda en bases de datos, etc. Si los alineamientos son erróneos, todos los resultados basados en los alineamientos están mal.

En el alineamiento de secuencias se distingue el alineamiento por pares, donde se alinean dos secuencias, del alineamiento múltiple, donde se utilizan más de dos secuencias. 

\subsection{Problema de la subsecuencia común más larga}
Los biólogos que encuentran una nueva secuencia genética suelen querer saber con qué otras secuencias es más parecida. Encontrar una subsecuencia es una forma de calcular el grado de similitud entre dos secuencias: cuanto más larga es la subsecuencia, más similares son. Los caracteres de una subsecuencia, a diferencia de los de una subcadena, no tienen por qué ser contiguos.

\subsection{Matriz de sustitución}
El problema del ADN es que puede mutar: mutación puntual de un nucleótido, inserción o deleción. Encontrar la similitud del ADN es importante, ya que secuencias similares pueden tener funciones similares. Uno de los algoritmos más ampliamente utilizados es BLAST. 

La matriz de puntuación de sustitución es una matriz bidimensional con valores de puntuación que describen la probabilidad de que un aminoácido o nucleótido haya sido reemplazado durante la evolución de la secuencia. Se pueden penalizar la apertura de huecos y la mutación puntual, creando una matriz ponderada. Se da una puntuación positiva a los pares de caracteres idénticos o similares y una puntuación negativa a los diferentes. Esta puntuación se basa en las frecuencias observadas de tales ocurrencias en alineaciones de ADN/proteínas relacionadas evolutivamente.

El parámetro más crítico en la comparación de secuencias es la elección de una matriz de puntuación/sustitución. Para una evolución reciente, se suele utilizar una matriz de identidad, ya que se espera que las secuencias no hayan divergido mucho. Para una evolución antigua, la matriz converge a un modelo aleatorio. En bioinformática hay varias matrices: PAM, BLOSUM, Gonnet, JTT.

\subsection{Matrices de puntuación}
\paragraph{PAM}
PAM viene de Point Accepted Mutation. Contiene la probabilidad derivada empíricamente de que se acepte una sustitución, basada en proteínas estrechamente relacionadas. Los números PAM más altos corresponden a una mayor distancia evolutiva. Cuando dos secuencias tienen una diferencia de 1 PAM, significa que una secuencia se puede transformar en la otra con una media de mutaciones puntuales de un 1\% (una mutación por cada 100 aminoácidos).

\paragraph{BLOSUM}
BLOSUM viene de Blocks Substitution Matrix. Es otra matriz derivada empíricamente, basada en proteínas relacionadas a mayor distancia. Los números BLOSUM más bajos corresponden a una mayor distancia evolutiva.

La comparación entre PAM y BLOSUM, a un nivel comparable de sustituciones, indica que los dos tipos de matrices producen resultados similares

\subsection{Needleman-Wunsch - alineamiento global}
Este método realiza un alineamiento global entre dos secuencias. Se inicializa en 0, y se rellena la matriz con el valor máximo entre la mutación puntual o el hueco (ya sea en una secuencia o en la otra) mediante la matriz de puntuación. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/needleman-wunsch.png}
\end{figure}

Una vez calculada la matriz F, la esquina inferior derecha de la matriz es la puntuación máxima de cualquier alineación. Para calcular qué alineación da realmente esta puntuación, se puede empezar por la celda inferior derecha y comparar el valor con las tres fuentes posibles (Opción1, Opción2 y Opción3) para ver de cuál procede.
\begin{itemize}
\item Si es la opción 1, entonces las dos secuencias están alineadas.
\item Si es la opción 2, entonces la primera secuencia está alineada con un hueco.
\item Si es la opción 3, entonces la segunda secuencia está alineada con un hueco.
\end{itemize}

El alineamiento global suele utilizarse para alinear secuencias que tienen aproximadamente la misma longitud y que ya se sabe que están relacionadas.

\subsection{Smith-Waterman - alineamiento local}
En este caso, se busca la mejor puntuación de los alineamientos de regiones de secuencias. De esta forma, se encuentran segmentos conservados, en lugar de alinear la secuencia completa. La diferencia con Needleman-Wunsch es la posibilidad de coger 0 como posibilidad a la hora de rellenar la matriz.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/smith-waterman.png}
\end{figure}

Tanto el algoritmo de Needleman-Wunsch como el de Smith-Waterman utilizan los conceptos de matriz de sustitución/puntuación, función de penalización por hueco y proceso de rastreo. 

\begin{table}[h]
\begin{mdframed}[backgroundcolor=black!10]
\textbf{Ejercicio:} encuentra el mejor alineamiento local entre TCAGTTGCC y AGGTTG con +1 para match, -2 para mistmatch y -2 para gap.

\begin{tabular}{c c c c c c c c c c c}
&&T&C&A&G&T&T&G&C&C\\
&0&0&0&0&0&0&0&0&0&0\\
A &0 &0&0&1&0&0&0&0&0&0\\
G &0 &0&0&0&2&0&0&0&0&0\\ 
G &0 &0&0&0&3&1&0&2&0&0\\ 
T &0 &1&0&0&1&4&1&3&1&0\\ 
T &0 &2&0&0&0&5&5&6&2&0\\ 
G &0 &0&0&0&2&1&3&6&5&3\\ 
\end{tabular}

Por tanto, la parte común es GTTG.

\textbf{Ejercicio:} lo mismo que antes, pero para el mejor alineamiento global.
\begin{tabular}{c c c c c c c c c c c}
&&T&C&A&G&T&T&G&C&C\\
&0&-2&-4&-6&-8&-10&-12&-14&-16&-18\\
A &-2 &-2&-4&-3&-5&-7&-9&-11&-13&-15\\
G &-4 &-4&-6&-5&-2&-4&-6&-8&-10&-12\\ 
G &-6 &-6&-8&-7&-4&-3&-5&-5&-7&-9\\ 
T &-8 &-4&-6&-7&-5&-2&-2&-4&-6&-8\\ 
T &-10 &-6&-6&-8&-6&-3&-1&-3&-5&-7\\ 
G &-12&-8&-8&-8&-5&-5&-3&0&-2&-4\\ 
\end{tabular}
Por tanto, el resultado es:\\
TCAGTTGCC\\
AG-GTTG
\end{mdframed}
\end{table}

%\section{Búsqueda en bases de datos}