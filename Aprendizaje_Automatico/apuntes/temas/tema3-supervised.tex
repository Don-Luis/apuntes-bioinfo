%24/02 - Aythami Morales 
\chapter{Aprendizaje supervisado}
\section{Introducción}
El aprendizaje automático se divide en dos grandes categorías: aprendizaje supervisado y aprendizaje no supervisado. En el aprendizaje supervisado, el objetivo es aprender una \textbf{función de hipótesis} desconocida que permita predecir una salida $y$ a partir de una entrada $x$. Este enfoque se conoce como sistema \textbf{data-driven}, ya que el modelo aprende patrones a partir de datos etiquetados.

\subsection{Datos etiquetados}
En el aprendizaje supervisado, cada dato tiene una etiqueta asociada, lo que se representa como pares $(x^{(i)}, y^{(i)})$. Partimos de un \textbf{conjunto de datos de entrenamiento}:
$$D_{train} = \{ (\vec{x}_n, y_n)\}^{N_{train}}_{n=1}$$
siendo:
\begin{itemize}
\item $N_{train}$: número de muestras de entrenamiento.
\item $\vec{x}_n$: vector de atributos (inputs, características, variables independientes).
\item $y_n$: etiqueta o valor objetivo.
\end{itemize}

El objetivo es \textbf{inducir o aprender} a partir de los datos de entrenamiento un \textbf{predictor} $h$ que permita predecir $y$ para nuevos datos. La clave no es memorizar los datos de entrenamiento, sino \textbf{generalizar} para hacer predicciones precisas en situaciones similares pero no idénticas (capacidad de generalización).

\subsection{Función de hipótesis y predictor}
El algoritmo de aprendizaje $L$ toma los datos de entrenamiento y busca una función $h$ que sirva como predictor:
$$L:D_{train} \rightarrow h$$
\begin{itemize}
\item $h$: función de hipótesis que mapea entradas $x$ a salidas $y$.
\item El predictor debe ser capaz de generalizar, es decir, funcionar bien con datos no vistos durante el entrenamiento.
\end{itemize}

\subsection{Tipos de problemas en aprendizaje supervisado}
\paragraph{Clasificación}
En un problema de clasificación, hay un espacio que se divide en las distintas etiquetas que adquieren los datos. No se trata de buscar la línea divisoria entre los distintos elementos, si no la caracterización de los espacios en los que se encuentran. Esa línea divisoria, o frontera de decisión, es importante para dicha caracterización. La salida es discreta, ya que se trata de etiquetas categóricas. Puede ser binaria (dos clases) o multiclase, pero nunca un dato podrá estar entre dos etiquetas. Además, las etiquetas pueden seguir un orden (etiqueta 1 < etiqueta 2 < etiqueta 3) o no.

\paragraph{Regresión}
Un ejemplo es la predicción del tamaño de un tumor en función del tiempo que lleva desarrollándose mediante la descripción de la tasa de crecimiento. En este caso, los datos son el tiempo (distinto número de semanas) y las etiquetas el tamaño del tumor, y se busca describir el tamaño para datos nuevos, como pueden ser semanas no descritas. Aun así, para una misma semana, puede haber distintas tasas de crecimiento dependiendo de la persona. En este caso sí se busca encontrar la recta. Puede haber varias soluciones independientes, y en este caso la salida es continua (un valor numérico).

El número máximo de clases a partir de que un problema pasa de clasificación a regresión depende de la naturaleza del problema. Si el problema es de naturaleza continua, entonces la aproximación será mediante regresión, mientras que si el problema es discreto, entonces utilizaremos la clasificación. Si existe una relación entre muestras consecutivas, se espera que una distancia entre muestras esté correlacionada o haya consecuencialidad. 

En resumen, en problemas de clasificación, el espacio de características se divide en regiones según las etiquetas. Por ejemplo, en un problema binario, se busca una frontera de decisión que separe las dos clases. En regresión, se busca una función (como una recta) que ajuste los datos.

\subsection{Función de pérdida}
Hasta ahora, seguíamos la siguiente notación:
\begin{itemize}
\item $x$, $y$: son variables aleatorias con una distribución conjunta, es decir, hay un patrón que permite predecir una variable en función de la otra.
\item Predictor $h$: permite obtener $y$ a partir de $x$.
\end{itemize}

La \textbf{función de pérdida} $L$ mide la discrepancia entre la predicción $h(x)$ y la etiqueta real $y$. El objetivo es minimizar esta pérdida. Existen dos tipos comunes de funciones de pérdida:
\begin{enumerate}
\item Clasificación
\begin{itemize}
\item Pérdida 0-1: 
$$L(h(x),y) = \mathbb{I}(h(x) \neq y)$$

donde $\mathbb{I}$ es una función indicadora que vale 1 si la predicción es incorrecta y 0 si es correcta. Así, el error se calcula como la suma de las predicciones incorrectas. Un sistema que acierte mucho tendrá un indicador $\mathbb{I}$ cercano a 0, mientras que un sistema que falle mucho tendrá un $\mathbb{I}$ alto que habrá que normalizar por los intentos realizados.
\end{itemize}

\item Regresión
\begin{itemize}
\item Error cuadrático medio (MSE):
$$MSE = \mathbb{E}[|h(\vec{X}) - Y|^2]$$

\item Error absoluto medio (MAE):
$$MAE = \mathbb{E}[|h(\vec{X}) - Y|]$$

\item El MSE magnifica los errores grandes debido al cuadrado, mientras que el MAE es más robusto y mantiene el sentido físico-biológico de la variable.
\end{itemize}
\end{enumerate}

\subsection{Pérdida esperada y generalización}
La pérdida esperada es un estimador que mide el rendimiento del modelo en toda la población, no solo en los datos de entrenamiento. Esto se debe a que los datos de entrenamiento se definen en una subregión o espacio finito de la región de todos los posibles valores. Se busca que el entrenamiento tenga un rendimiento bueno con datos conocidos y desconocidos. Se define como:
$$Error = \mathbb{E}[\mathbb{I}(h(\vec{X}) \neq Y)]$$

El objetivo es minimizar la pérdida esperada, lo que implica que el modelo generalice bien a datos nuevos.

\subsection{Entrenamiento y optimización}
El entrenamiento consiste en ajustar los \textbf{parámetros} $\Theta$ del modelo para minimizar la pérdida en los datos de entrenamiento. La pérdida promedio en el conjunto de entrenamiento se define como:
$$L_{train}(\Theta) = \frac{1}{N_{train}} \sum^{N_{train}}_{n=1} L(h(\vec{x}^{train}_n; \Theta), y^{train}_n)$$
\begin{itemize}
\item $\Theta$: parámetros del modelo (por ejemplo, coeficientes en regresión lineal).
\item $L_{train}$: pérdida promedio en el conjunto de entrenamiento.
\end{itemize}

Sin embargo, minimizar $L_{train}$ no garantiza un buen rendimiento en datos nuevos. Esto se debe al \textbf{sobreajuste} (overfitting), donde el modelo memoriza los datos de entrenamiento pero no generaliza bien a datos no vistos.

Para evitar el sobreajuste, se introduce una \textbf{penalización de complejidad} en la función de pérdida. Esto limita el número de parámetros o la magnitud de los mismos, favoreciendo modelos más simples y generalizables. Ejemplos comunes incluyen la regularización L1 (Lasso) y L2 (Ridge) que penalizan la suma absoluta o cuadrática de los parámetros respectivamente.

\subsubsection{Sesgos en el entrenamiento}
La pérdida de entrenamiento suele ser menor que la pérdida en datos nuevos, ya que el modelo se optimiza específicamente para los datos de entrenamiento. Esto introduce un \textbf{sesgo} en el proceso de entrenamiento. Para evaluar el rendimiento del modelo en datos no vistos, se utiliza un \textbf{conjunto de test}:

Los datos de test no se usan para ajustar $\Theta$, solo para evaluar el rendimiento. La pérdida en el conjunto de test se calcula de manera similar a $L_{train}$, pero sin modificar $\Theta$.

%25/02 - Aythami
\paragraph{Underfitting}
El tipo de predictor considerado tiene \textbf{poca capacidad expresiva} o pocos grados de libertad. En consecuencia, no es capaz de capturar las dependencias entre los atributos y la variable a predecir. La pérdida esperada del predictor es demasiado elevada. Esto suele ocurrir en modelos rígidos, como pueden ser los modelos lineales.

\paragraph{Overfitting}
El tipo de predictor considerado es \textbf{demasiado flexible} (demasiados grados de libertad) y aprende patrones espurios que no son relevantes para la predicción (por ejemplo, fluctuaciones de muestreo, ruido, valores atípicos, etc.). La estimación de entrenamiento de la pérdida esperada es demasiado optimista y subestima la pérdida esperada real. Esto puede ocurrir en modelos flexibles, como pueden ser las redes neuronales.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/under-overfitting.png}
\end{figure}

\subsection{Compensación entre sesgo y varianza}
Se tiene un sesgo bajo en modelos flexibles que se ajustan bien a los datos de entrenamiento, pero pueden tener alta varianza (sobreajuste). El sesgo alto se suele dar en modelos rígidos que no se ajustan bien a los datos, pero tienen baja varianza (underfitting). Por ello, se describe la compensación. A medida que aumenta la complejidad del modelo, el error de entrenamiento disminuye, pero el error de test puede aumentar después de un punto óptimo debido al sobreajuste.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/tradeoff.png}
\caption{En el eje x está la complejidad del modelo, o el orden del polinomio. En el eje y está la predicción del error. Se empieza con un error alto, y se va adaptando, mediante modelos más complejos, para disminuir el error. No obstante, llega un momento en el que, aunque el error de entrenamiento siga bajando con modelos cada vez más complejos, el error de test vuelve a aumentar, indicando un modelo sobreajustado.}
\end{figure}

\subsection{Arquitectura, parámetros}
\begin{itemize}
\item \textbf{Arquitectura, configuración}: Especificación de la forma funcional del predictor (por ejemplo, número y tipo de capas ocultas y neuronas en una red neuronal, elección del núcleo en una SVM, etc.). En el caso de la regresión polinomial, es el polinomio.
\item \textbf{Parámetros del modelo} $\Theta$: Valores necesarios para especificar el sistema de predicción (por ejemplo, pesos sinápticos en una red neuronal, vectores de soporte en una SVM, etc.). Se determinan entrenando el modelo con datos etiquetados.
\item \textbf{Configuración del algoritmo de aprendizaje}: Función a optimizar (por ejemplo, verosimilitud, probabilidad posterior, error cuadrático medio, etc.). Términos de regularización
\end{itemize}

Hay distintos tipos de modelos predictivos: vecinos cercanos, árboles de decisión, redes neuronales, Support Vector Machine.

\subsection{Preprocesado de datos}
El preprocesado es crucial para preparar los datos antes de entrenar un modelo. Incluye:
\begin{enumerate}
\item \textbf{Selección de características}: identificar las características más relevantes
\item \textbf{Manejo de valores atípicos (outliers)}: detectar y tratar datos anómalos
\item \textbf{Manejo de datos faltantes}: imputar o eliminar valores faltantes
\item \textbf{Normalización}: centrar y escalar los datos para que tengan media 0 y desviación estándar 1. La normalización basada en la media y desviación estándar es más robusta que usar valores mínimos y máximos.
\end{enumerate}

El preprocesado se puede realizar programáticamente con el módulo de scikit-learn.

\subsection{Medición del rendimiento}
\subsubsection{Error de clasificación}
El error de clasificación se estima como la proporción de veces que el modelo predice incorrectamente las etiquetas. Es una métrica básica para evaluar la efectividad del modelo.

\subsubsection{Matriz de confusión}
La matriz de confusión es una herramienta fundamental para evaluar el rendimiento en problemas de clasificación. Muestra las combinaciones entre las etiquetas reales y las predichas. En clasificación binaria, se divide en:
\begin{itemize}
\item \textbf{True Positive (TP)}: correctamente clasificados como positivos.
\item \textbf{True Negative (TN)}: correctamente clasificados como negativos.
\item \textbf{False Positive (FP)}: incorrectamente clasificados como positivos.
\item \textbf{False Negative (FN)}: incorrectamente clasificados como negativos.
\end{itemize}

En problemas con más de dos clases, la matriz de confusión se extiende, y la diagonal principal indica las predicciones correctas, mientras que las demás celdas muestran los errores. Esto es útil para identificar qué clases se confunden con mayor frecuencia.

\subsubsection{Métricas de rendimiento}
A partir de la matriz de confusión, se derivan varias métricas para evaluar el rendimiento del modelo:
\begin{itemize}
\item \textbf{Accuracy (exactitud)}: mide la efectividad general del modelo.
$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

\item \textbf{Error}: estimación de la proporción de clasificación incorrectas.
$$Error = 1 - Accuracy = \frac{FP + FN}{TP + TN + FP + FN}$$

\item \textbf{Sensitividad (Recall)}: ratio de positivos correctamente identificados.
$$Sensitividad = \frac{TP}{TP + FN}$$

\item \textbf{Especifidad}: ratio de negativos correctamente identificados.
$$Especificidad = \frac{TN}{TN + FP}$$

\item \textbf{Precisión}: ratio de predicciones positivas que son correctas.
$$Precision = \frac{TP}{TP + FP}$$
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/medidas.png}
\caption{En la literatura se habla de Non Target Comparison en lugar de Negativos por la connotación. Normalmente hay un cierto solape entre las distribuciones target y non-target, produciendo así un error. El umbral de decisión se coloca dependiendo del error que se quiera priorizar (o si no se quiere priorizar ninguno).}
\end{figure}

\subsubsection{Curva ROC y AUC}
La curva ROC es una gráfica la tasa de verdaderos positivos (TPR; sensitividad) frente a la tasa de falsos positivos (FPR) para diferentes umbrales de decisión. La matriz de confusión solo tiene sentido cuando ya están las etiquetas y se ha definido un umbral, pero esto no es necesario para la curva ROC. 

El AUC (Área bajo la curva) mide el rendimiento general del clasificador. Un AUC cercano a 1 indica un buen rendimiento, mientras que un AUC cercano a 0.5 sugiere un rendimiento aleatorio.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/roc.png}
\end{figure}

La curva ROC es útil para decidir el umbral de decisión óptimo, especialmente cuando se quiere priorizar la minimización de falsos negativos o falsos positivos.

\subsubsection{Métricas para regresión}
En problemas de regresión, las métricas comunes incluyen:
\begin{itemize}
\item \textbf{Error cuadrático medio (MSE)}:
$$MSE = \mathbb{E}[(h(\vec{X}; \Theta) - Y)^2] \approx \frac{1}{N} \sum^N_{n = 1}(h(\vec{x}_n;\Theta) - y_n)^2$$

\item \textbf{Error absoluto medio (MAE)}:
$$NMSE = \frac{MSE}{Var}$$
$$Var = \frac{1}{N} \sum^N_{n = 1} (y_n - \bar{y})^2$$

\item \textbf{Error cuadrático medio normalizado (NMSE)}:
$$MAE = \mathbb{E}[|h(\vec{X};\Theta) - Y|] \approx \frac{1}{N} \sum^N_{n = 1} |h(\vec{x}_n; \Theta) - y_n|$$
\end{itemize}

\subsection{Protocolos experimentales}
En general, tenemos un conjunto de datos que debemos dividir nosotros en dos conjuntos, uno de train y uno de test. 

\paragraph{Holdout}
Se divide el conjunto de datos en dos partes: una para entrenamiento (por ejemplo, 70\%) y otra para test (30\%). Problema: La partición puede no ser representativa si los datos no están bien distribuidos.

\paragraph{K-Fold Cross Validation}
El conjunto de datos se divide en $K$ subconjuntos (folds). El modelo se entrena $K$ veces, utilizando 
$K-1$ folds para entrenamiento y 1 fold para validación. El rendimiento final es el promedio de las $K$ iteraciones. Ventaja: Reduce la dependencia de una partición específica y proporciona una estimación más robusta del rendimiento.

\paragraph{Leave-One-Out}
 Caso extremo de K-Fold, donde $K=N$ (número de muestras). En cada iteración, se deja una muestra fuera para validación y se entrena con las $N-1$ restantes. Ventaja: Útil cuando el conjunto de datos es muy pequeño. Desventaja: Computacionalmente costoso y puede estar sesgado si hay outliers.

%25/02 - Daniel Hernández-Lobato
\section{Teoría de decisión}
En un problema de aprendizaje automático, trabajamos con datos observados (atributos) y etiquetas de clase. Por ejemplo:
\begin{itemize}
\item \textbf{Atributos}: una radiografía de un paciente.
\item \textbf{Etiquetas de clase}: si el paciente está sano o enfermo.
\end{itemize}

El objetivo es realizar \textbf{predicciones}: dado un nuevo paciente, asignarle una etiqueta (enfermo o sano) basándose en sus atributos. Para ello, asumimos que los datos observados se generan a partir de una \textbf{distribución de probabilidad conjunta} sobre los atributos y las etiquetas de clase. Una vez observados los atributos, utilizamos esta distribución para tomar decisiones sobre la asignación de etiquetas.

A partir de los datos observados, \textbf{inferimos} la distribución de probabilidad conjunta. Utilizamos la distribución inferida para asignar etiquetas a nuevos datos.

\subsection{Probabilidad condicional y regla de Bayes}
Para asignar una etiqueta a un nuevo dato, nos interesa la probabilidad condicional de la etiqueta dado los atributos observados, $p(C|x)$, donde:
\begin{itemize}
\item $C$: etiqueta de clase (por ejemplo, enfermo o sano).
\item $x$: atributos observados (por ejemplo, radiografía).
\end{itemize}

La \textbf{regla de Bayes} permite calcular esta probabilidad condicional:
$$p(C|x) = \frac{p(x|C) \cdot p(C)}{p(x)}$$
donde
\begin{itemize}
\item $p(x|C)$: probabilidad de observar los atributos $x$ dada la clase $C$ (verosimilitud).
\item $p(C)$: probabilidad a priori de la clase $C$ (por ejemplo, la fracción de la población que está enferma).
\item $p(x)$: probabilidad marginal de los atributos $x$ (constante de normalización).
\end{itemize}

Entre las reglas de probabilidad hay dos fundamentales:
\begin{itemize}
\item \textbf{Regla del producto}: la probabilidad conjunta de $x$ y $C$ es el producto de la verosimilitud y la probabilidad a priori.
$$p(x, C) = p(x|C) \cdot p(C)$$

\item \textbf{Regla de la suma}: la probabilidad marginal de $x$ se obtiene sumando (marginalizando) sobre todas las clases.
$$p(x) = p(x, C_1) + p(x, C_2)$$
\end{itemize}

\subsubsection{Ejemplo de probabilidades condicionales}
Consideremos un ejemplo con dos atributos: color de ojos y color de pelo. La siguiente tabla muestra las frecuencias observadas:
\begin{table}[h]
\centering
\begin{tabular}{ll|ll|}
\cline{3-4}
                                            &        & \multicolumn{2}{l|}{Pelo}         \\ \cline{3-4} 
                                            &        & \multicolumn{1}{l|}{Rojo} & Rubio \\ \hline
\multicolumn{1}{|l|}{\multirow{2}{*}{Ojos}} & Marrón & \multicolumn{1}{l|}{5}    & 10    \\ \cline{2-4} 
\multicolumn{1}{|l|}{}                      & Azul   & \multicolumn{1}{l|}{10}   & 20    \\ \hline
\end{tabular}
\end{table}

A partir de esta tabla, podemos calcular varias probabilidades
\begin{itemize}
\item Probabilidad marginal:
$$p(\text{ojos marrones}) = \frac{15}{45}$$

\item Probabilidad conjunta:
$$p(\text{ojos marrones, pelo rubio}) = \frac{10}{45}$$

\item Probabilidad condicional:
$$p(\text{ojos marrones}|\text{pelo rubio}) = \frac{10}{30}$$
\end{itemize}

\subsubsection{Probabilidad posterior para dos clases}
En un problema con dos clases ($C_1$ y $C_2$) se cumple que:
$$p(C_1|x) + p(C_2|x) = 1$$
Esto se debe a que, según la regla de Bayes:
$$p(C_1|x) = \frac{p(x|C_1) p(C_1}{p(x)}$$
$$p(C_2|x) = \frac{p(x|C_2) p(C_2}{p(x)}$$
Sumando ambas probabilidades:
$$p(C_1|x) + p(C_2|x) = \frac{p(x|C_1) \cdot p(C_1) + p(x|C_2) \cdot p(C_2)}{p(x)} = \frac{p(x)}{p(x)} = 1$$

\subsubsection{Ejercicios teorema de Bayes}
Estamos trabajando para una empresa que ha desarrollado una nueva prueba para detectar una enfermedad. La prueba se caracteriza por tener una alta probabilidad de dar positivo en personas enfermas. Sin embargo, a veces puede fallar como se indica en la siguiente tabla:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
        Test output & Is the person sick? & Probability \\ \hline
        + & Sick & 0.9 \\ \hline
        - & Sick & 0.1 \\ \hline
        + & Healthy & 0.05 \\ \hline
        - & Healthy & 0.95 \\ \hline
    \end{tabular}
\end{table}

Sean las dos etiquetas de clase $\mathcal{C}_0 \equiv \text{Healty}$ y $\mathcal{C}_1 \equiv \text{Sick}$ y $x$ el resultado de la prueba, es decir, las variables observadas. Por lo tanto, la tabla anterior proporciona todos los valores potenciales para las distribuciones condicionales de clase $p(x|\mathcal{C}_k)$.

\paragraph{Escenario 1} 
Supongamos que tenemos la misma probabilidad a priori de que una persona esté enferma o sana, es decir, $p(\mathcal{C}_1)=p(\mathcal{C}_0)=0,5$. 

\textit{Utiliza el teorema de Bayes para calcular la fracción de personas que están realmente enfermas a partir del número total de personas que son positivas según la prueba. Es decir, se pide calcular $p(\mathcal{C}_1|x=+)$.}
$$p(sick|+) = \frac{p(+|sick) \cdot p(sick)}{p(+)} =$$
$$ \frac{p(+|sick) p(sick)}{p(+, sick) + p(+, healthy)} = \frac{p(+, sick) p(sick)}{p(+|sick)p(sick) + p(+|healthy) p(healthy)}$$

Ahora rellenamos con los datos del enunciado y la tabla
$$\frac{0.9 \cdot 0.5}{0.9 \cdot 0.5 + 0.05 \cdot 0.5} = 0.95$$

\textit{Según los resultados obtenidos, ¿es muy buena la prueba desarrollada? ¿Por qué?}
Parece un buen test, ya que si es positivo, con un 95\% de probabilidad la persona realmente está enferma.

\paragraph{Escenario 2}
Supongamos ahora que la enfermedad es muy rara y que sólo afecta al 1\% de la población. Es decir, $p(\mathcal{C}_1)=0,01$. 

\textit{Utilice el teorema de Bayes para calcular la fracción de personas que están realmente enfermas a partir del número total de personas que son positivas según la prueba. Es decir, se le pide que calcule $p(\mathcal{C}_1|x=\text{+})$.}

$$p(sick|+) = \frac{p(+|sick) \cdot p(sick)}{p(+)} = $$
$$\frac{p(+|sick) p(sick)}{p(+, sick) + p(+, healthy)} = \frac{p(+, sick) p(sick)}{p(+|sick)p(sick) + p(+|healthy) p(healthy)}$$

Ahora rellenamos con los datos del enunciado y la tabla:
$$\frac{0.9 \cdot 0.01}{0.9 \cdot 0.01 + 0.05 \cdot 0.99} = 0.15$$

\textit{Según los resultados obtenidos, ¿es muy buena la prueba desarrollada? ¿Por qué? ¿Qué es lo que falla en la prueba en este escenario?}

Este test es malo, ya que con un test positivo, la probabilidad de estar enfermo es bastante baja. El problema está en los falsos positivos. Para mejorar el test, hay que reducir la probabilidad de los falsos positivos.

\textit{Calcula cuál es la tasa de falsos positivos necesaria, es decir, $p(x=\text{+}|\mathcal{C}_0)$, de la prueba para que al menos el 90\% de las personas que den positivo en la prueba estén realmente enfermas.}

Ahora queremos calcular $p(+ | healthy)$:
Sabemos que 
$$p(sick|+) = \frac{p(+|sick) \cdot p(sick)}{p(+)} = 0.9$$
y que:
$$p(+) = p(+, sick) + p(+, healthy)$$

Esto es igual a :
$$p(+) = p(+|sick) p(sick) + p(+|healthy) p(healthy)$$

Sustituyendo en función de la tabla del enunciado:
$$p(+) = 0.9 \cdot 0.01 + p(+|healthy) \cdot 0.99$$

Insertando esto en la primera fórmula:
$$0.9 = \frac{ p(+|healthy) \cdot p(sick)}{p(+)} = \frac{0.9 \cdot 0.01}{0.9 \cdot 0.01 +  p(+|healthy) \cdot 0.99}$$

Se puede pasar la parte de abajo de la fracción al otro lado:
$$0.9 \cdot 0.9 \cdot 0.01 + 0.9 \cdot p(+|healthy) \cdot 0.99 = 0.9 \cdot 0.01$$

Simplificando
$$0.9 \cdot p(+|healthy) \cdot 0.99 = 0.9 \cdot 0.01 - 0.9 \cdot 0.9 \cdot 0.01 $$
$$ p(+|healthy) = \frac{0.9 \cdot 0.01 - 0.9 \cdot 0.9 \cdot 0.01}{0.9 \cdot 0.99} = 10^{-3}$$
 
\paragraph{Escenario 3}
\textit{Supongamos ahora que disponemos de $T$ pruebas independientes para diagnosticar la enfermedad, pero que tienen las mismas características que las descritas en la tabla anterior. Como las pruebas son independientes, tenemos que }
$$ p(x_1=\text{+},x_2=\text{+},\ldots, x_T = \text{+}|\mathcal{C}_k)=\prod_{i=1}^T p(x_i=\text{+}|\mathcal{C}_k)$$
\textit{¿En cuántas de estas pruebas tiene que dar positivo una persona para que al menos el 90\% de las personas diagnosticadas como positivas estén realmente enfermas?}
$$p(sick | x_1 = +, ..., x_T = +) = 0.9$$

Se busca $T$. Tenemos la siguiente pista: $log(a^b) = b \cdot log(a)$ y $log(ab) = log(a) + log(b)$
$$p(sick | x_1 = +, ..., x_T = +) = p(+ | sick)^T = 0.9^T$$
$$p(sick | x_1 = +, ..., x_T = +) = 0.9 $$
$$0.9 = \frac{p(x_1 = +, ..., x_T = + | sick) \cdot p(sick)}{p(x_1 = +, ..., x_T = + | sick) \cdot p(sick) + p(x_1 = +, ..., x_T = + | healthy) \cdot p(healthy)}$$
$$p(healthy | x_1 = +, ..., x_T = +) = p(+ | healthy)^T = 0.05^T$$
$$0.9 = \frac{0.9^T \cdot 0.01}{0.9^T \cdot 0.01 + 0.05^T \cdot 0.99}$$
$$0.9 \cdot 0.9^T \cdot 0.01 + 0.9 \cdot 0.05^T \cdot 0.99 = 0.9^T \cdot 0.01$$
$$0.9 \cdot 0.05^T \cdot 0.99 = 0.9^T \cdot 0.01 - 0.9 \cdot 0.9^T \cdot 0.01$$
$$0.9 \cdot 0.05^T \cdot 0.99 = 0.9^T[0.01 - 0.9 \cdot 0.01]$$

Aplicando los logaritmos:
$$log 0.9 + T \cdot log 0.05 + log 0.99 = T \cdot log 0.9 + log(0.01 - 0.9 \cdot 0.01)$$
$$T log 0.05 - T log 0.9 = log(0.01 - 0.9 \cdot 0.01) - log 0.9 - log 0.99$$
$$T(log 0.05 - log 0.9) = \frac{log(0.01 - 0.9 \cdot 0.01) - log 0.9 - log 0.99}{log 0.05  log 0.9} = 2.3$$

%04/03 - Daniel 
\subsection{Minimizar el porcentaje de errores de clasificación}
Una regla de clasificación dividirá el espacio de entrada (de atributos x) en regiones $\mathcal{R}_k$. Todos los $\vec{x} \in \mathcal{R}_k$ se asignan a $\mathcal{C}_k$. Los límites se denominan superficies de decisión o fronteras de clasificación. La probabilidad de error es:
\begin{align*}
p(mistake) &= p(\vec{x} \in \mathcal{R}_1, \mathcal{C}_2) + p(\vec{x} \in \mathcal{R}_2, \mathcal{C}_1) \\
& = \int_{\mathcal{R}_1} p(\vec{x}, \mathcal{C}_2) d\vec{x} + \int_{\mathcal{R}_2} p(\vec{x}, \mathcal{C}_1) d \vec{x}
\end{align*}
Si el ejemplo cae en la región 1, pero pertenece a la clase 2, se produce un error de clasificación. Lo mismo al contrario: si un dato cae en la región 2 y realmente viene de la clase 1, también se produce un error de clasificación. El error que se comete es el área debajo de las funciones de arriba. Para minimizar la probabilidad de error, hay que construir la siguiente regla de predicción:
$$\pi(\vec{x}) = \begin{cases}
\mathcal{C}_1 if p(\vec{x}, \mathcal{C}_1) \geq p(\vec{x}, \mathcal{C}_2) \\
\mathcal{C}_2 if p(\vec{x}, \mathcal{C}_2) \geq p(\vec{x}, \mathcal{C}_1) 
\end{cases}
$$
Esta regla es justo la que predice la clase que tiene la mayor probabilidad posterior $p(\mathcal{C}_k | \vec{x})$.

Para minimizar el error de predicción y conseguir el clasificador con menor error, se debe predecir la etiqueta de clase con mayor probabilidad posterior. A este clasificador se le conoce como \textbf{clasificador de Bayes} al utilizar el teorema de Bayes.

El problema es que las probabilidades posteriores no se conocen, y se deben inferir de los datos. Se deben utilizar distintos métodos para estimar estas probabilidades posteriores.

Consideramos que tenemos un problema de clasificación en cuyo eje x se encuentra el espacio posible de los datos. Hay dos distribuciones posibles: $p(\mathcal{C}_1|x)$ y $p(\mathcal{C}_2|x)$. Tenemos tres puntos: A, B y C. ¿Dónde tendríamos que poner la frontera de clasificación? Respuesta: es donde se cortan las probabilidades posteriores, que en este caso es el punto B.

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/example-distributions.jpg}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/missclassification.png}
\caption{Imaginamos que tenemos un problema de clasificación de una dimensión. Tenemos dos clases: la positiva y la negativa. Queremos especificar un clasificador para resolver el problema. Definimos las regiones $\mathcal{R}_1$ y $\mathcal{R}_2$ para las clases $\mathcal{C}_1$ y $\mathcal{C}_2$ respectivamente. Para un dato que se observe en la región 1, pero pertenezca a la clase 2, se debe calcular el área debajo de la curva representado en rojo y verde. Si x cae en la región 2 y realmente viene de la clase 1, el error se calcula como el área azul. El clasificador no es óptimo, ya que la frontera de clasificación no es muy correcta. Si se cambiase al punto marcado con las líneas discontinuas, el error se vería minimizado. Con el cambio de frontera, si x viene de la clase 2 y cae en la región 1, el error sería el área verde que se sale de la frontera. Si x cae en la región 2 y pertenece a la clase 1, el error es el área verde de la derecha sumado al área azul. Generalmente, el clasificador óptimo se encuentra cuando las probabilidades posteriores de clase tomen el mismo valor. En la línea discontinua se cortan las dos curvas de distribución de ambas clases. Las probabilidades posteriores de clase es igual a la probabilidad conjunta dividida por una constante. Si las probabilidades conjuntas toman el mismo valor, las probabilidades posteriores también son iguales. En ese punto donde las probabilidades posteriores valen 0.5 se encuentra la frontera de clasificación.}
\end{figure}

\subsection{Minimizar la pérdida esperada}
A veces, tomar decisiones equivocadas puede tener costes asimétricos.
\begin{itemize}
\item Paciente que no tiene cáncer al que se le diagnostica cáncer: estrés
\item Paciente con cáncer diagnosticado que no tiene cáncer: muerte prematura
\end{itemize}
Estas cuestiones pueden formalizarse mediante una matriz de pérdidas L:
$$L_{kj} \equiv \text{Cost of assigning } \mathcal{C}_j \text{ to } \vec{x} \text{ with true class } \mathcal{C}_k$$
$$\vec{L} = \begin{pmatrix}
0 & 1 \\ 10 & 0
\end{pmatrix}$$

Las filas contienen la clase verdadera, y las columnas representan la clase etiquetada. La diagonal tiene 0, ya que es donde estaríamos acertando (se predice la etiqueta de clase correcta). Como en ese caso no se comete ningún error, se pone 0, y los elementos fuera de la diagonal indican los errores que se cometen. En este caso, tiene más peso que a un paciente con cáncer se le etiquete como no cáncer.

Nuestro objetivo es minimizar la pérdida esperada:
$$\mathbb{E}[loss] = \sum_k \sum_j L_{kj} p(\vec{x} \in \mathcal{R}_j, \mathcal{C}_k) = \sum_k \sum_j L_{kj} \int_{\mathcal{R}_j} p(\vec{x}, \mathcal{C}_k) d \vec{x}$$

Debemos elegir $\mathcal{R}_j$ para minimizar $\sum_k L_{kj} p(\vec{x}, \mathcal{C}_k)$. La regla de decisión es asignar $\vec{x}$ a $\mathcal{C}_j$ para el que $\sum_k L_{kj} p(\mathcal{C}_k | \vec{x})$ sea el mínimo.

Si tenemos acceso a las probabilidades posteriores, podemos tomar decisiones de manera óptima y tener en cuenta posibles asimetrías de los errores. Si tuviéramos el mismo peso para los errores, la frontera de clasificación esperada sería la calculada anteriormente (0.5).

\subsection{Opción de rechazo}
A veces es mejor evitar las decisiones sobre los casos difíciles, es decir, regiones en las que la mayor $p(\mathcal{C}_k | \vec{x})$ es significativamente menor que 1. En el caso de las radiografías, dejamos que un humano clasifique los casos más ambiguos. 

Se introduce un umbral $\theta$ y se rechazan todos aquellos datos $\vec{x}$ para los cuales $max (p(\mathcal{C}_k | \vec{x}) < \theta$. Si $\theta = 1$, todos los ejemplos se rechazan. Si $\theta < 1/K$, ningún ejemplo se rechaza.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/rechazo.png}
\caption{La región de rechazo se encuentra alrededor de la frontera de decisión.}
\end{figure}

\subsection{Inferencia y decisión}
Hay dos procesos diferentes:
\begin{itemize}
\item Inferencia: utiliza datos de entrenamiento para estimar las probabilidades posteriores de clase, es decir, modelar $p(\mathcal{C}_k | \vec{x})$
\item Decisión: utilizar las probabilidades posteriores para hacer asignaciones óptimas de clase
\end{itemize}

Generalmente, los distintos clasificadores son distintos mecanismos para estimar las probabilidades posteriores. Hay tres tipos de clasificadores en función de su complejidad. Por orden descendente, son:
\begin{itemize}
\item Modelos generativos: coge los ejemplos de cada clase y estima las distribuciones condicionales (de la clase positiva y de la clase negativa). Una vez estimadas, y sabiendo la probabilidad a priori de cada clase en base a los datos de entrenamiento, se utiliza la regla de Bayes para clasificar los datos nuevos.
El problema es que $\vec{x}$ puede ser muy dimensional (sobre todo en biología y salud, como muchos genes u otros atributos) y es posible que necesitemos muchos datos para determinar las densidades con una precisión razonable. Esto se debe a la maldición de la dimensionalidad. Conforme haya más dimensiones, el espacio está cada vez más vacío, por lo que es más difícil estimar la densidad. Permite generar nuevos datos. No obstante, estima p(x), lo que resulta útil para detectar valores atípicos. Es caro si sólo queremos decisiones de clasificación.

\item Modelos discriminativos: se estima directamente las probabilidades posteriores de clase. Tiene un enfoque mucho más sencillo que el anterior al no tener que estimar una probabilidad posterior en múltiples dimensiones. Las densidades condicionales de clase pueden contener mucha estructura que tiene poco efecto en las probabilidades posteriores.

\item Discriminantes: se estima una función que mapea $\vec{x}$ al espacio de etiquetas. Su enfoque es aún más sencillo, combinando el paso de inferencia y decisión en uno solo. La pega es que ya no tenemos acceso a $p(\mathcal{C}_k|\vec{x})$, por lo que la minimización de la función de pérdida y la opción de rechazo no son posibles.
\end{itemize}

Las densidades condicionales de clase complicadas pueden dar lugar a probabilidades posteriores de clase simples.
\begin{figure}[h]
\centering
\includegraphics[width = 0.9\textwidth]{figs/inference-decision.png}
\caption{Suponemos que tenemos una dimensión con una clase positiva y una clase negativa. La distribución es complicada al tener dos modas. Si esto se utiliza en la regla de Bayes, se pueden calcular las probabilidades posteriores, las cuales tienen menos complejidad que las densidades. Generalmente, las probabilidades posteriores son funciones relativamente sencillas y más fáciles de calcular que las distribuciones condicionales.}
\end{figure}

\subsection{Modelos generativos}
Cuando un clasificador era un modelo generativo, se estiman las probabilidades a priori. La clave del uso de modelos generativos para la clasificación es especificar una forma adecuada para las densidades condicionales de clase. Elegiremos una distribución con parámetros $\theta$. La tarea consiste en estimar el mejor valor de $\theta$ a partir de los datos observados.

Supongamos que $x \in [1, 2, 3, \ldots, 100]$ y consideramos dos posibles distribuciones:
$$\theta_{two} \equiv \text{distribución uniforme entre las potencias de dos}$$
$$\theta_{even} \equiv \text{distribución uniforme entre los números pares}$$

Observamos los valores de x en $\mathcal{D} = [16, 8, 2, 64]$. Estos números son pares y también se pueden generar por potencias de dos. ¿Qué distribución se elegiría? La de potencias de dos, ya que es más probable que esa sea la distribución verdadera.

\subsubsection{Principio de máxima verosimilitud}
Los datos se han generado independientemente. La observación de los datos para cada distribución es:
$$p(\mathcal{D}|\theta_{two}) = \prod_{i=1}^N p(x_i | \theta_{two}) = (1/6)^N = 7.7 \cdot 10^{-4}$$
$$p(\mathcal{D}|\theta_{even}) = \prod_{i=1}^N p(x_i | \theta_{even}) = (1/50)^N = 1.6 \cdot 10^{-7}$$

La probabilidad es mayor en la distribución de las potencias de dos. 
El principio de máxima verosimilitud elige el modelo que maximice la probabilidad de los datos observados.

\begin{align*}
\hat{\theta}_{ML} &= arg max_{\theta}  \hspace{1cm} p(\mathcal{D} | \theta) = \prod^N_{i=1} p(\vec{x}_i|\theta) \\
&= arg max_{\theta} \hspace{1cm} p(\mathcal{D} | \theta) = \sum^N_{i=1} p(\vec{x}_i|\theta)
\end{align*}

Para estimar los parámetros, se utiliza la máxima verosimilitud, eligiendo la media y varianza para ello. Los valores de los parámetros que maximizan la probabilidad de los datos es mediante los estimadores de máxima verosimilitud, que es la media empírica y la varianza empírica.

Estos estimadores son asintóticamente insesgados. Convergen a los valores reales a medida que tenemos más y más datos.

Ejemplo: suponemos que los datos observados han surgido de una distribución gaussiana de la cual no conocemos la media ni la varianza. Hemos observado los datos de x1 a xn. Se puede escribir la probabilidad de los datos observados dado la media y la varianza: 
$$p(\mathcal{D} | \mu, \sigma^2) = \prod^N_{i = 1} p(x_i | \mu, \sigma^2)$$
Esto se puede convertir en:
\begin{align*}
\log p(\mathcal{D} | \mu, \sigma^2) &= \sum^N_{i = 1} p(x_i | \mu, \sigma^2) \\
&= \sum^N_{i = 1} -0.5 \log 2 \pi \sigma^2 - \frac{1}{2 \cdot \sigma^2} (x_i - \mu)^2
\end{align*}
$$\log p(D|\mu) = \sum^N_{i = 1} - \frac{1}{2 \sigma^2} \frac{(x_i - \mu)^2}{\sigma^2} + cte$$
\begin{align*}
\frac{\delta \log p(D|\mu)}{\delta \mu} &= \sum^N_{i=1} \frac{-2(x_i - \mu)}{2 \sigma^2} \cdot -1 + 0 = 0 \\
&= \sigma^N_{i=1} \frac{x_i  \mu}{\sigma^2} = 0 \\
&= \sum^N_{i=1} (x_i) - \mu) = 0 \\
&= \sum^N_{i=1}(x_i) - N \mu = 0 \\
&= \sum^N_{i = 1} x_i = N \mu \\
\end{align*}
$$\mu = \frac{1}{N} \sum^N_{i=1} x_i$$

En múltiples dimensiones, en lugar de tener una varianza, hay una matriz de covarianzas. Esto se puede ver mejor en el notebook de MaximumLikelihoodGaussian (carpeta ejercicios).

\paragraph{Sobreajuste}
El principio de máxima verosimilitud puede llevar a un \textbf{sobreajuste severo} cuando hay pocos datos. El sobreajuste $p(x|\hat{\theta}_{ML}$ captura propiedades de los datos que no generaliza bien. Esto se traduce en que la probabilidad para los datos de entrenamiento es muy alta, pero para datos nuevos muy baja. Por ejemplo, si en una distribución gausiana solo tenemos un valor observado, el estimador de la media que maximiza la probabilidad es el mismo. La media se pondría donde está la observación, y la varianza sería 0, ya que la diferencia entre la media y nuestro dato es 0. Esto se traduce en que la gaussiana es muy picuda y se centra en el dato observado. Si la varianza es 0, la probabilidad de los datos observados es infinita (siendo esto lo mejor que hay). Esto parece muy bueno según el principio de máxima verosimilitud. No obstante, para un dato nuevo, la densidad de probabilidad de un dato nuevo, diferente al dato observado, sería 0. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.9\textwidth]{figs/overfitting-ml.png}
\end{figure}

\subsubsection{Ejercicios}
\begin{enumerate}
\item Un problema de clasificación binaria tiene probabilidades condicionales de clase:
$$p(\mathcal{C}_1 | x) = \frac{1}{1 + exp(-w x)} \hspace{2cm} p(\mathcal{C}_2 | x) = \frac{1}{1 + exp(w x)} $$
siendo $w$ algún parámetro que puede valer cualquier número excepto 0. El objetivo está en encontrar la frontera de decisión óptima.

$$\frac{1}{1 + exp(-w x)} = \frac{1}{1 + exp(w x)} \hspace{1cm} | \text{invertir fracción} $$
$$1 + exp(-w x) = 1 + exp(w x) \hspace{1cm} | -1$$
$$exp(-w x) = exp(w x) \hspace{1cm} | \log$$
$$-w x = w x $$
$$w x + wx = 0$$
$$2wx = 0$$
$$x = 0$$

\end{enumerate}
%\section{Análisis discriminante}

%\section{Árboles de clasificación y vecinos cercanos}