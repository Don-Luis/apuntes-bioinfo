---
title: "Bacterial composition determined by 16S MetaBarcoding"
subtitle: "1. ASVs Determination and filtering"
author: "Sandra Mingo Ramírez"
date: "Febrero 2025"
output: 
  html_document: 
    toc: yes
---

## Introduction (explicación del experimento)

This is a Notebook containing all the pipeline described in [DADA2 tutorial](https://benjjneb.github.io/dada2/tutorial.html). We are using data from article with doi https://doi.org/10.1186/s12866-019-1572-x . This work analyzes the influence of soybean rhizosphere on bacterial communities both in agriculture and forest soil. 16S rRNA gene based bacteria profiling were accomplished with MiSeq 275 bp paired-end sequencing targeted V3-V4 regions, with forward primer 341F = 5′-CCTACGGGNGGCWGCAG-3′ (17bps) and reverse primer 785R = 5′-GACTACHVGGGTATCTAATCC-3 (21 bps). Amplicon size around 445 nts.

Data was downloaded from BioProject PRJNA474716.

## Libraries needed for the project

```{r loading_libraries , message=FALSE, cache=TRUE }
#Recomendable poner un título a cada bloque de código
library(dada2) #librería con la que se determinan SV
library (ggplot2)
library(plotly) #librería que genera una máscara encima de cualquier objeto ggplot para que el gráfico sea interactivo
library(DECIPHER) #analizar disparidad entre secuencias
library(ShortRead) #interpretación archivos fasta
library(Biostrings)
library(readr) #parte de tidyverse para la interpretación de dataframes
library("phyloseq") #integración de datos y hacer todos los gráficos
library("DESeq2") #permite ver qué bacterias son las enriquecidas en una muestra con respecto a otra; calcular diversidad
library("tibble")
library("dplyr")
library("tidyr")
library(openssl) #for the codification of ASVs nucleotide sequence into a shorter string (ASVs names)
library("vegan") #hacer estudios de diversidad
library("microbiome") #estudios de diversidad
library("hrbrthemes") #scales in plots of ggplot
library("RColorBrewer") #paletas de colores de degradados
library("data.table")#conversiones en dataframes que no son posibles en dplyr
```

## Raw fastq files ubication

First we check the name of the files with the 16S sequence using list.files. "pattern" uses regular expression to select the files we are interested in. We assume files have format SAMPLENAME_1.fastq and SAMPLENAME_2.fastq. We are going to use the forward reads only, so _1.fastq.

```{r checking_sequences , message=FALSE, cache=TRUE }
list.files(path="./fastq/", pattern ="_1.fastq")
```

If it's working we can generate a vector with forward file names *fnFs* and another one with reverse file names *fnRs*. We also generate a vector with sample names *sample.names*. We are going to use the last one to name the files after trimming and filtering.

```{r selecting_files, message=FALSE, cache= TRUE}
#filename Forwards
fnFs <- sort(list.files(path="./fastq", pattern="_1.fastq", full.names = TRUE))
#filename Reverse
fnRs <- sort(list.files(path="./fastq", pattern="_2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_1.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

print("Forward Files list")
fnFs
print("Reverse Files list")
fnRs
print("Sample names")
sample.names
```

## Removing Primers

As we are analysing PCR products primers sequence can introduce a technical bias in the 5' and 3' region of the amplicon as replaces the biological source sequences. We are going to use Cutadapt software (installed in a conda environment) to remove any occurrences in the sequences.

Este paso quita los cebadores de la PCR.

### Cutadapt software path definition

We have to specify cutadapt ubication so that we can run the program. So let's write down the path of the program (in this case */home/condapython/anaconda3/env/cutadaptenv/bin*) and check with system2 whether we can run it.

Como cutadapt es un programa externo a R, se va a hacer una llamada. 

```{r cutadapt_path}
cutadapt <-"/home/sandra/miniconda3/envs/meta-env/bin/cutadapt"
system2(cutadapt, args = "--version")
```

### Primer sequence definition

Let's define the primers and their corresponding reverse complement sequences. We also define a function called allOrients to obtain all combinations. This is done easily using Biostrings package, so we must convert sequences in string variables to DNAString object and the result must be converted back into a string using toString function.

```{r primers}
FWD<- "CCTACGGGNGGCWGCAG"
REV <- "GACTACHVGGGTATCTAATCC"
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings) #si Biostrings no está cargado, no funciona
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

```

Next, we have to check whether those sequences are appearing or not in our reads. We define first primerHits function.

```{r primer_count}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    # fn = filename
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
```

Según esto, la mayoría de las lecturas van a tener en Forward el primer de forward y en Reverse los primers de reverse. Además, a la hora de preparar la librería, han cogido el adaptador de Illumina y han fusionado el cebador en forward. Lo mismo con el reverse. Así, en el primer paso de PCR ya han añadido los adaptadores de Illumina. 

Now we are going to prepare a directory to contain the trimmed sequences after running cutadapt. New files are going to be named after sample.name value (check above). Sometimes, when using degenerated primers with IUPAC wildcards, some primers are not recognized as such, that's why it's recommended to use "--match-read-wildcards" option.

```{r cutadapt}
# Create an output directory to store the clipped files
cut_dir <- file.path(".", "cutadapt")
if (!dir.exists(cut_dir)) dir.create(cut_dir)

fnFs.cut <- file.path(cut_dir, basename(fnFs))
fnRs.cut <- file.path(cut_dir, basename(fnRs))

FWD.RC <- dada2:::rc(FWD) #reverse complement de forward
REV.RC <- dada2:::rc(REV) #reverse complement de reverse

names(fnFs.cut) <- sample.names
names(fnRs.cut) <- sample.names
#por esto era importante antes ordenar de forma alfabética

#Define minimum length of reads to keep after trimming

minlen <- 150 
#se podría haber dejado en 200. Dada2, en algún momento, debe solapar forward y reverse de al menos 20 nucleótidos. Por tanto, poniendo 150 nucleótidos, hay 300 en total y el amplicón es de más de 400. Esto no nos vale y metemos secuencias de más, pero estas las filtramos después. Lo importante aquí es ver si pueden solapar.

# It's good practice to keep some log files so let's create some
# file names that we can use for those 
cut_logs <- path.expand(file.path(cut_dir, paste0(sample.names, ".log")))

cutadapt_args <- c("-g", FWD, "-a", REV.RC, 
                   "-G", REV, "-A", FWD.RC,
                   "--match-read-wildcards", #importante para la notación IUPAC, si no se pone se perderán la mayoría de las lecturas
                   "-n", 3, "--discard-untrimmed", "--minimum-length", minlen)

# Loop over the list of files, running cutadapt on each file.  If you don't have a vector of sample names or 
# don't want to keep the log files you can set stdout = "" to output to the console or stdout = NULL to discard
for (i in seq_along(fnFs)) {
  system2(cutadapt, 
          args = c(cutadapt_args,
                   "-o", fnFs.cut[i], #salida de los forward
                   "-p", fnRs.cut[i], #salida para los reverse
                   fnFs[i], fnRs[i]), #inputs
          stdout = cut_logs[i])  
}

#cutadapt necesita analizar forward y reverse a la vez siempre y cuando sea el mismo elemento

# quick check that we got something
head(list.files(cut_dir))
```

Let's look for the presence of adapters in cut files

```{r cutadapt check}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

## Filtering trimmed reads according to quality

Once we have removed primers we are going to filter reads according to quality. We are going to use the filterAndTrim function from dada2 package.

### Inspect read quality profiles

First we must check each file quality profile using *plotQualityProfile* function from dada2 package. The resulting plot is a ggplot object and we can add( an interactive layer using *plotly::ggplotly*. As we have many files we are going to aggregate them to get a single plot with the average values for each file (aggregate parameter in function stablished as true).

```{r forwardfilesquality, message=FALSE, cache=TRUE}
forwplot<-ggplotly(plotQualityProfile(fnFs.cut[1:length(fnFs.cut)], aggregate=TRUE) + 
                     geom_hline(yintercept=c(15,25,35), 
                                color=c("red","blue","green"), 
                                size=0.5),
                   width =600) 
forwplot

```

La calidad empieza a bajar sobre la posición / ciclo 236. También hay que tener en cuenta que los primeros nucleótidos tienen un pequeño descenso. La línea de abajo es el número de lecturas que llegan. Hay una pequeña asimetría en la calidad.

In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line).

The forward reads are good quality. We generally advise trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed. As sequence in 5' is low quality and in order to remove primers we will start from position **7** and we will truncate the forward reads at position **255** (trimming the last 10 nucleotides).

Now we visualize the quality profile of the reverse reads:

```{r reversefilesquality, warning = FALSE, message=FALSE, cache=TRUE}
revqplot<-ggplotly(plotQualityProfile(fnRs.cut[1:length(fnRs.cut)], aggregate=TRUE) + 
                     geom_hline(yintercept=c(15,25,35), 
                                color=c("red","blue","green"),
                                size=0.5),
                   width =600)
revqplot
```

As expected, reverse sequences have less quality and we are going to fix the position **234** where the quality drops below 26 and we get most of the reads.

### Filter and trim

We generate a vector with the filenames for the filtered fastq.gz files. Files will be saved in "cutfiltered" folder. Again, we use sample.names as reference.

```{r namefilterfiles, warning = FALSE, cache = TRUE}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(".", "cutfiltered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(".", "cutfiltered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Filtering will be achieved using *filterAndTrim* function. We'll use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2. The maxEE parameter sets the maximum number of "expected errors" allowed in a read, which is a better filter than simply averaging quality scores. Additionaly we are going to trim the first seven nucleotides of the forward reads and establish the cutting point in 3' in 255 and 234 as indicated above. We are going to use the *multithread* parameter to speed up the process. NOTE: Windows R version cannot parallelice the process, so multithread must be set to FALSE if running this code in Windows.

```{r filtering, warning = FALSE, cache=TRUE}
out <- filterAndTrim(fnFs.cut, filtFs, fnRs.cut, filtRs, 
              maxN=0, maxEE=c(2,5), trimLeft=c(7,0), truncLen = c(255,234), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
out<- cbind(out, perc.cons=round(out[, "reads.out"]/out[, "reads.in"]*100, digits=2))
print("Total Reads")
sum(out[,2])
```

**Considerations for your own data**: The standard filtering parameters are starting points, not set in stone. If you want to speed up downstream computation, consider tightening maxEE. If too few reads are passing the filter, consider relaxing maxEE, perhaps especially on the reverse reads (eg. maxEE=c(2,5)), and reducing the truncLen to remove low quality tails. Remember though, when choosing truncLen for paired-end reads you must maintain overlap after truncation in order to merge them later.

Las decisiones tomadas para cortar las secuencias son bastante arbitrarias. Se podría crear un algoritmo para tomar decisiones más coherentes mediante un análisis estadístico. La recomendación es fijarnos en el valor de calidad de un Phred26 para ver en qué momento decae la calidad de las lecturas. Se procura analizar el mismo set de muestras del mismo pool o ronda de secuenciación. Normalmente, todo esto se podría analizar a la vez, pero como hay muchas variabilidades técnicas, podemos encontrar ese problema. 

Nos han quedado 1.700.000 lecturas. ¿Son suficientes para caracterizar la población? El amplicón tiene un tamaño mayor que el solapamiento entre forward y reverse. El paso final es reconstruir todo el amplicón con esas secuencias, por lo que hay que mantener 20 nucleótidos. Si perdemos demasiadas lecturas, se puede modificar el parámetro de truncLen y expected error. 

## Dereplication and denoising

Once we have a nice pool of reads, DADA2 protocol is going to analyse them to identify the unique sequences and to correct the errors. This process is called dereplication and denoising. The output of this process is a table with the number of reads for each unique sequence. This table is called *sequence table* and it is the input for the next step: chimera detection.

Este paso es crucial. A partir de este proceso tendremos la población in silico que representa la población biológica. Como tenemos que hacer un filtrado del ruido técnico, el primer paso es utilizar la función de learnErrors. Va a analizar las lecutras forward, alinearlas entre sí y en función del score del alineamiento calcular la probabilidad de cambio de un nucleótido en una posición dada y si es por un error de secuenciación o por una mutación puntual. Este proceso se corre por un lado para la lectura forward y por otra para la reverse. Esto se debe a que hay un mayor rango de errores en las lecturas reverse, como se vio en el gráfico del FastQC. El siguiente paso está basado en Machine Learning. En este caso, el set de entrenamiento no está validado. Si los cambios aplican al modelo, se acepta el modelo subyacente de DADA o se generan modificaciones. Si se hace con todos los datos, se sobreajustaría y sería un gran gasto computacional.

### Learn the Error Rates

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors). By default learnErrors is using 1+e8 nucleotides to infer the model, but depending in the number of samples or computer memory this value can be modified. Nevertheless the lower the number, the less reliable the model will be.

::: {#dadashortcut style="color: yellow"}
*IMPORTANT DO NOT RUN THE TWO FOLLOWING CHUNKS* As we are using a not very powerful VM and class time is limited, we are going to load the error models from the previous execution. Instead of running next chunks we are going to load the models that have been saved in the file *dadaobjects.Rdata*
:::

```{r errForward, cache = TRUE, eval=TRUE}
#Modelo para las lecturas forward
errF <- learnErrors(filtFs,multithread=TRUE, nbases=130000000) #half of total bases
#Coge como input los archivos filtrados
#Multithread TRUE no sirve para Windows, habría que cambiarlo
#nbases es una decisión que hay que tomar para ver cuántos nucleótidos se van a analizar. Se recomienda la mitad o un cuarto de los nucleótidos en la población muestral, depende de las ganas que tengamos a esperar. Cuanto más pequeño sea el modelo, menor diversidad y menor fiabilidad. 
#Hay un parámetro que indica si coge las lecturas al azar. Esto sería interesante para no coger solo lecturas de los primeros archivos, si no de todos.
```

```{r errReverse, cache = TRUE, eval=FALSE}
#Modelo para las lecturas reverse
errR <- learnErrors(filtRs, multithread=TRUE, nbases=130000000)
```

```{r load_dada_results, cache = TRUE}
load("dadaobjects.RData")
```

### Plot Error models

We can visualize the error rates for each possible transition (A→C, A→G, ...) and the estimated error rates after convergence of the machine-learning algorithm. The error rates are expected to drop with increased quality as the Phred quality score increases. The plotErrors function will plot the error rates for each possible transition, with the observed error rates for each consensus quality score shown as points, the estimated error rates after convergence of the machine-learning algorithm shown as a black line, and the error rates expected under the nominal definition of the Q-score shown as a red line.

```{r ploterrorsF, message= FALSE, warning= FALSE, cache = TRUE}
plotErrors(errF, nominalQ=TRUE)
```

```{r ploterrorsR, message= FALSE, warning= FALSE, cache = TRUE}
plotErrors(errR, nominalQ=TRUE)
```

We can see in both plots that the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

En este caso, se deberían subir el número de bases y lecturas a evaluar. Esto se debe a que los puntos no se ajustan del todo a las líneas rojas. No se aprecia el valle visto en Novaseq. Para automatizar, se haría un bloque de código para estimar las bases en todas las lecturas y aquellas que se deben pasar por learn errors para llegar a la mitad (o 25%) en filtFS/filtRS. Podemos ver en algunos casos que la recta al final se curva un poco hacia arriba. Esto no debería suceder y se debe a un mal ajuste.

::: {#novaseq style="color: yellow"}
Depending on the sequencing platfform used the error models we can find anomalies. For instance Novaseq system does not return a continuous scale for Phred quality values. See example below:

![Novaseq error model](novaseq.png){fig-align="left"}
Esto es una matriz en la que se muestran todos los posibles cambios de transiciones y transversiones del ADN. Se representa el score del consenso para cada cambio y la frecuencia de error en base logarítmica. A mayor consenso, menor error. Los cambios entre  sí misma no tiene apenas errores, que  es lo esperable. Para los otros casos, hay una recta y unos puntos que siguen más o menos este patrón. Esto es un caso de algo que no se recomienda. Illumina tiene múltiples máquinas para la secuenciación. La última es Novaseq, pero su problema es que hace un binning de las calidades. Esto se debe a que, en un punto, hay una alteración en la calidad y hay un valle. Al aplicar estos errores, se pierden casi todas las lecturas.
:::

### Sample Inference

We are now ready to apply the [core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods) to the filtered and trimmed sequence data.

::: {#dadashortcut style="color: yellow"}
*IMPORTANT DO NOT RUN THE TWO FOLLOWING CHUNKS* As we are using a not very powerful VM and class time is limited, we are going to use the objects that were loaded with _dadaobjects.RData_ file.
:::

```{r dadaF, warning=FALSE, cache=TRUE, eval=FALSE}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```

```{r dadaR, warning=FALSE, cache=TRUE, eval=FALSE}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

Inspecting the returned dada-class object:

```{r dadaview, warning=FALSE, cache=TRUE}
print("Forward Reads")
dadaFs
#en la muestra 7265228 se han encontrado 3366 secuencias con mutaciones o lecturas distintas. Podríamos decir que se trata de SVs, pero todavía no vamos a decirlo porque se debería validar con las lecturas reversas.
print("Reverse Reads")
dadaRs
```

## Merge paired reads

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged "contig" sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least **20** bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

Ahora tenemos las lecturas forward y reverse y queremos solaparlas para obtener los SV. Esto se consigue con mergePairs, que toma como input los variantes que ha detectado en forward y reverse y las lecturas filtradas. Solapará y contará las veces que aparece el solapamiento en cada archivo. Así se obteienen las secuencias de cada SV y su abundancia.

```{r merging, message=FALSE, warning=FALSE, cache=TRUE}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]][,2:9])
```

El resultado es una tabla con la abundancia, los mismatches, indels, etc.

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged \$sequence, its \$abundance, and the indices of the \$forward and \$reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

::: {#additionalinfo style="color: yellow"}
**Considerations for your own data**: Most of your reads should successfully merge. If that is not the case upstream parameters may need to be revisited: Did you trim away the overlap between your reads?

**Extensions**: Non-overlapping reads are supported, but not recommended, with mergePairs(..., justConcatenate=TRUE). Check for ITS. No todos los análisis tienen amplicones tan definidos. Algunos ITS tienen una extensión y longitud muy variable, siendo más grande que la información que nos da una librería de Illumina. Por ello, no merece la pena forzarles a que solapen. Se puede utilizar entonces mergePairs para concatenar las secuencias.
:::

## Construct Sequence Table

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r seqtable , warning = FALSE, cache = TRUE}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
head(colnames(seqtab)) #como nombre de columna está una secuencia nucleotídica de la ASV. Antiguamente, se ponía OTU1, OTU2, OTU3, etc. Al correr dos experimentos, la secuencia de OTU1 de un análisis no tiene por qué coincidir con la del otro, por lo que, con la secuencia, no es necesario nombrarlas. Dependiendo de la librería puede merecer o no la pena poner la secuencia, ya que la secuencia puede ser muy larga para poder visualizarse bien. Esto se podría codificar con un hash como MD5.
```

```{r inspecttable, warning = FALSE, cache = TRUE}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. This table contains around 16000 ASVs, but not all the lengths of our merged sequences all fall within the expected range for this V3-V4 amplicon (445 nts) minus primers removed with cutadapt (38 nts). We are going to keep those from 393 to 433.

Como nombre de columna hay un número, y después está el valor (la abundancia). Alrededor de 394 empieza a crecer el valor de la abundancia, hasta 432. Técnicamente, podríamos quitar los ASV de aquellos que tengan un tamaño que no sea real tras haber cortado las secuencias. Puede haber algún amplicón raro con un tamaño grande, pero solo hay una ASV con ese tamaño. Por tanto, hay que filtrar por tamaño. Esto no se recomienda hacer porque posteriormente vamos a filtrar por abundancia y prevalencia. 

```{r filteringseq, warning = FALSE, cache = TRUE}
seqtab<- seqtab[,nchar(colnames(seqtab)) %in% 393:432]
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))               
```

La tabla anterior se ha reducido bastante. De esta forma, el siguiente paso es más ágil.

## Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant "parent" sequences.

Las quimeras eran artefactos que surgen en ciclos inacabados de PCR. Dependiendo del ciclo en el que aparezca este error, su efecto es  mayor o menor. Se espera que cadaa molécula en la PCR se duplique. Si la eficiencia es de 1, eso ocurre, si es inferior, no todas las moléculas se replican. Normalmente, siempre hay  alguna que no se replique, pero los valores estándares están sobre el 90%.

Antiguammente había uuna base de datos de secuencias de 16S que se utilizaba como comparador para definir las quimeras (si eran una mezcla de dos entradas en esta base de datos). En este caso, comparamos solo con los individuos de nuestra  población mediante el consenso.

```{r chimeras, warning=FALSE, cache=TRUE }
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
print("Removing chimera")
dim(seqtab.nochim)
print("Percentage against original sequences")
sum(seqtab.nochim)/sum(seqtab)*100
```

**WARNING**: Check filtering step if number of sequences drops drastically.

Teníamos quimeras, pero nos quedamos con el 94.9% de las secuencias.

## Track reads through the pipeline

As a final check of the progress, we'll look at the number of reads that made through each step in the pipeline (o cuántas vamos perdiendo). Así tenemos una idea de en qué punto hemos perdido información para volver a empezar. El profesor  prefiere verlo todo al final y ver toda  la evolución, no anticiparse y obligarse a repetir. Otro criterio puede ser hacerlo tras cada paso, ya que son largos.

```{r pipeline_summary, warning=FALSE, cache=TRUE}
getN <- function(x) sum(getUniques(x))
track <- cbind(out[,1:2], sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track<- as.data.frame(track)
track$conservedperc<-round(with(track, nonchim/input*100),2)
print(track)
```

Este código crea un dataframe con la primera muestra (de forma representativa). Se añade una últimaa columna del porcentaje de lecturas conservadas no quiméricas. Vemos que para todas las  muestras, los porcentajes de secuencias  conservadas se mantiene.

Se pueden hacer subOTUs a partir de estos para ver la homología.

## Removing low abundance ASVs

DADA is quite restrictive when estimating ASVs, however there is a chance that low abundant sequences are kept, being some of them due to artifacts or an underrepresented part of the community that can add noise to diversity study. According to the literature it's recommended to remove those ASVs with a number of sequences below the one per mil of the average number of sequences per sample. Let's have a look to track\$nonchim distribution

```{r track_distribution}

summary(track$nonchim)

```

Aquí se quieren quiitar las ASV de baja abundancia y prevalencia. Normalmente, la bubliografía recomienda que, asumiendo una calidad de 30 media, hay un error cada mil nucleótidos. Teniendo una media de 75574 secuencias, suponemos 75 secuencias erróneas. La resoolución de las relaciones taxonómicas no varía, pero la calidadd sí aumenta tras eliminar las secuencias con baja  abundancia y prevalencia. 

So with summary(track\$nonchim)\[\['Mean'\]\]' we get the average and with round(summary(track\$nonchim)\[\['Mean'\]\]/1000,0) we will get our threshold value.

Besides the abundance threshold we can add a value for presence threshold, that is, number of samples where the ASV is present. As we are working with triplicates we can chose a value of three for threshold. Teniendo réplicas, se esperaría que un ASV aparezca en al menos dos muestras. Aunn así, como hay muestras triplicadas, se puede poner un valor de 3. Se agruparían las muestras por las réplicas y se  filtraría. Hay librerías que lo hacen con los metadatos, no sería necesario crear una función para eso.

So now we can write a function for filtering

```{r abundance_filtering_function}
subset_columns_by_sum_and_rows <- function(df_name, threshold, min_rows_with_value) {
  df <- df_name # get dataframe by name
  
  # compute the sum of each column
  col_sums <- colSums(df)
  
  # identify columns with sum above the threshold
  columns_to_keep <- col_sums >= threshold
  
  # iterate over each column to check if it meets the threshold criteria
  for (col_name in names(df)) {
    if (!columns_to_keep[col_name]) next # skip columns that are already marked for removal
    if (sum(df[, col_name] > 0) < min_rows_with_value) { # check if the column meets the threshold criteria
      columns_to_keep[col_name] <- FALSE # mark the column for removal
    }
  }
  
  # subset dataframe to keep columns with sum above the threshold and the minimum number of rows with non-zero values
  df_new <- df[, columns_to_keep]
  
  # output the new dataframe
  return(df_new)
}
```

Now we can filter with our parameters

```{r low_abundance_filtering}
#raretons
seqtab.nochimfiltered<- subset_columns_by_sum_and_rows(seqtab.nochim, 
                                                       threshold=round(summary(track$nonchim)[['Mean']]/1000,0),
                                                       min_rows_with_value=3)
track$nochimfiltered<-rowSums(seqtab.nochimfiltered)
track$filteredperc<-round(with(track, nochimfiltered/nonchim*100),2)
print(track)
```

## Exporting sequences

Assuming that we want to start with dada2 in R and move to taxonomy assignments and different analysis in qiime2 (e.g following q2 tutorials like Moving Pictures etc.). First we have to export results, table, representative sequences and stats:

```{r export_resutls, warning=FALSE, cache = TRUE}
#Let's codify feature names in MD5 to have similar names to those obtained in qiime2

seqtab.nochimmd5 <-seqtab.nochim
sequences <- colnames(seqtab.nochimmd5)
sequencesmd5<-md5(sequences)
colnames(seqtab.nochimmd5)<-sequencesmd5
write.table(t(seqtab.nochimmd5), "seqtab-nochim.txt", sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)
uniquesToFasta(seqtab.nochim, fout='rep-seqs.fna', ids=sequencesmd5)
write.table(t(track), "stats.txt", sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)

#The same code for the filtered table except stats.txt as it's the same
seqtab.nochimfilteredmd5<-seqtab.nochimfiltered
sequencesfiltered<-colnames(seqtab.nochimfilteredmd5)
sequencesfilteredmd5<-md5(sequencesfiltered)
colnames(seqtab.nochimfilteredmd5)<-sequencesfilteredmd5
write.table(t(seqtab.nochimfilteredmd5), "seqtab-nochimfiltered.txt", sep="\t", row.names=TRUE, col.names=NA, quote=FALSE)
uniquesToFasta(seqtab.nochimfiltered, fout='rep-seqs_filtered.fna', ids=sequencesfilteredmd5)
```
