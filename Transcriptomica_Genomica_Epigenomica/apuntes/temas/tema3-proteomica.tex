%19/02 - José Manuel Rodríguez
\part{Proteómica y Metabolómica}
\chapter{Introducción a la proteómica y la espectrometría de masas}
\section{Introducción}
El \textbf{proteoma} se define como el conjunto completo de proteínas que se expresan, o pueden expresarse, a partir del genoma de una célula, tejido u organismo en un momento y condición específicos. La \textbf{proteómica}, por su parte, es la disciplina científica que estudia el proteoma mediante técnicas sistemáticas para identificar, cuantificar y caracterizar proteínas. Entre las herramientas más utilizadas en proteómica se encuentran la electroforesis, la espectrometría de masas (MS), la resonancia magnética nuclear (RMN), la microscopía óptica y electrónica, y la espectroscopía infrarroja por transformada de Fourier, entre otras.

\subsection{Análisis de proteínas por electroforesis}
La electroforesis es una técnica clásica de separación de proteínas basada en su carga eléctrica y masa molecular. En este método, las proteínas se separan en un gel según su punto isoeléctrico (pI), que es el pH al cual una proteína tiene una carga neta cero. Posteriormente, se realiza una segunda separación en función del peso molecular. Esta técnica ha sido fundamental en la proteómica "clásica" y sigue utilizándose para la validación de biomarcadores.

Tras la separación, el gel puede cortarse para aislar las proteínas de interés, las cuales se someten a una digestión con proteasas (como la tripsina) para generar péptidos. Estos péptidos pueden analizarse posteriormente mediante espectrometría de masas.

Sin embargo, la electroforesis presenta limitaciones: no es automática, tiene baja reproducibilidad y no es adecuada para proteínas grandes o hidrofóbicas. Además, suele ser efectiva solo para proteínas altamente abundantes. Estas limitaciones llevaron al desarrollo de la proteómica "Bottom-Up", que supera muchos de estos problemas.

\section{Proteómica "Bottom-Up"}
La proteómica "Bottom-Up" es un enfoque moderno que se basa en la digestión de proteínas en péptidos, seguida de su análisis mediante cromatografía líquida acoplada a espectrometría de masas (LC-MS). Este método es más sensible, reproducible y adecuado para el análisis de proteínas de baja abundancia.

\subsection{Digestión tríptica}
El primer paso en la proteómica "Bottom-Up" es la digestión de las proteínas. Las proteínas, en su estado nativo, están plegadas y pueden contener enlaces disulfuro que estabilizan su estructura. Para facilitar su análisis, las proteínas se desnaturalizan utilizando agentes como el dodecilsulfato sódico (SDS), que rompe los enlaces disulfuro y despliega las proteínas. Una vez desnaturalizadas, se someten a una digestión con tripsina, una enzima que corta específicamente después de los residuos de lisina (K) y arginina (R), generando péptidos de tamaño adecuado para su análisis por espectrometría de masas.

\subsection{Fraccionamiento para reducción de complejidad}
Tras la digestión, los péptidos resultantes pueden fraccionarse para reducir la complejidad de la muestra. Esto es especialmente útil en muestras que contienen múltiples proteínas o especies. El fraccionamiento puede realizarse mediante técnicas como la cromatografía de fase reversa, donde los péptidos se separan según su hidrofobicidad. Este paso permite una introducción más controlada y ordenada de los péptidos en el espectrómetro de masas.

\subsection{Cromatografía líquida y espectrometría de masas}
Los péptidos, fraccionados o no, se introducen en un sistema de cromatografía líquida (LC). Aquí, los péptidos se separan en función de su interacción con la fase estacionaria, lo que permite su elución en tiempos específicos. A medida que los péptidos salen de la columna cromatográfica, se ionizan mediante técnicas como la ionización por electrospray (ESI), formando gotitas cargadas que contienen los péptidos ionizados. A medida que el solvente se evapora, los péptidos ionizados entran en el espectrómetro de masas.

\subsubsection{Componentes del espectrómetro de masas}
Un espectrómetro de masas consta de los siguientes componentes principales:
\begin{itemize}
\item \textbf{Sistema de introducción de muestras}: Introduce los péptidos ionizados en el espectrómetro.
\item \textbf{Cámara de ionización}: Aquí, los péptidos se ionizan. Una de las técnicas más comunes es la ionización por electrospray (ESI).
\item \textbf{Analizador}: Determina la relación masa-carga (m/z) de los iones. Existen diferentes tipos de analizadores, como los de cuadrupolo, tiempo de vuelo (TOF) y trampa de iones.
\item \textbf{Detector}: Registra la masa y la intensidad de los iones detectados.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = \textwidth]{figs/espectro-analisis.png}
\caption{La figura superior indica el cromatograma obtenido. Primero hay un tiempo de retención con la intensidad encontrada de toda la carga iónica recibidas. A partir de un tiempo de retención señalado con la línea roja, empieza el tiempo de adquisición. El MS1 (panel central) es la ampliación de la línea roja del cromatograma. Cada línea vertical indica la masa detectada con sus intensidades. La parte derecha es un zoom de la línea roja. El MS1 se genera por cada péptido encontrado. Para este ejemplo, se cogen los iones marcados en gris y rojo y se fragmentan. Se saca la masa, carga e intensidad por cada uno de los iones en el segundo analizador. }
\end{figure}

La siguiente imagen muestra una representación de un LC-MS. A partir de un determinado tiempo de retención, hay una serie de masas cargas de los péptidos con una intensidad asociada a cada uno. Para diferentes tiempos de retención hay diferentes picos del espectrómetro.

\begin{figure}[h]
\centering
\includegraphics[width = \textwidth]{figs/lc-ms.png}
\end{figure}

Se tienden a coger los picos con mayor intensidad, ya que el resto son ruido del espectrómetro. Se sabe que a la hora de tener los péptidos, hay unos iones que van del extremo N-terminal al C-terminal y otros que van en sentido contrario. Estos iones van calculando la masa acumulativa. De esta forma se pueden saber los aminoácidos que componen el espectro.

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/fragmentacion-msms.png}
\end{figure}

\subsection{Adquisición de datos}
La adquisición de datos en espectrometría de masas puede realizarse de dos formas principales:
\begin{itemize}
\item \textbf{Adquisición dependiente de datos (DDA)}: En este modo, los iones más abundantes detectados en el espectro MS1 se seleccionan para su fragmentación, generando espectros MS2. Este proceso se repite secuencialmente para múltiples iones.
\item \textbf{Adquisición independiente de datos (DIA)}: En este modo, se fragmentan regiones específicas del espectro MS1, independientemente de la intensidad de los iones. Esto permite la detección de iones de baja abundancia, aunque puede resultar en espectros MS2 más complejos debido a la co-fragmentación de múltiples péptidos.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/data-adquisition.png}
\end{figure}

\subsection{Identificación de péptidos y proteínas}
La identificación de péptidos y proteínas se realiza comparando los espectros experimentales con bases de datos teóricas. Estas bases de datos contienen información sobre las masas y secuencias de péptidos generados in silico a partir de proteínas conocidas. Al comparar los espectros experimentales con los teóricos, se puede determinar la secuencia de aminoácidos de los péptidos y, por tanto, identificar las proteínas presentes en la muestra. Cada identificación se asocia con un score de confianza que indica la fiabilidad del resultado.

\subsection{Aplicaciones de la proteómica "Bottom-Up"}
La proteómica "Bottom-Up" tiene dos enfoques principales:
\begin{itemize}
\item \textbf{Proteómica de descubrimiento (Discovery Proteomics)}: Se utiliza para analizar proteomas completos, identificando y cuantificando proteínas de abundancia moderada a alta. Un posible ejemplo es la búsqueda de posibles biomarcadores.
\item \textbf{Proteómica dirigida (Targeted Proteomics)}: Se centra en la cuantificación de proteínas específicas, incluso en bajas abundancias. Una técnica común en este enfoque es el monitoreo de reacciones seleccionadas/múltiples (SRM/MRM), que utiliza tres analizadores de cuadrupolo para seleccionar y cuantificar péptidos específicos. Un ejemplo de aplicación es la validación de biomarcadores.
\end{itemize}

\subsubsection{Cuantificación y estándares internos}
En la cuantificación de proteínas, es crucial utilizar estándares internos para corregir variaciones en la ionización y la eficiencia de la cromatografía. Estos estándares son péptidos sintéticos con propiedades similares a los péptidos de interés, pero marcados con isótopos estables. Al comparar las áreas bajo la curva de los péptidos de interés con las de los estándares internos, se obtiene una cuantificación precisa y reproducible.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/internal-standard.png}
\end{figure}

%Cuando la muestra es compleja, los péptidos pueden competir causando colución. La intensidad va a disminuir al haber menos iones que se cuantificarán. La cuantificación de una muestra compleja será por tanto menor que el mismo péptido en una muestra poco compleja. Por esto, se establecen estándares internos: péptides con cualidades similares marcados. Se conoce la cantidad a priori del péptido asociado. Así, da igual si la complejidad es mucha o poca, ya que se establece la relación entre el péptido asociado y el péptido buscado. Lo que se calcula es el ratio del pico, es decir, el área del péptido buscado con el área del estándar interno. 

%21/02 - José Manuel Rodríguez
\section{Proteómica "Top-Down"}
En este caso, tenemos la proteína intacta y se calcula la masa con el espectrómetro. Se usan los mismos tipos de espectrómetros: cromatografía líquida y espectrómetro de masas. Químicamente, la muestra biológica no se desnaturaliza ni se digiere, si no que se sigue un protocolo de lisis para romper las membranas celulares. Dependiendo de las muestras y sus condiciones, las proteínas mantendrán su estructura o si están haciendo ligando, se mantendrán las relaciones. Si para esas condiciones de muestra hay alguna modificación post-traduccional, va a ser más fácil de identificar con esta técnica de Top-Down. 

Las proteínas intactas se pasan a un HPLC. Se realiza la ionización por electrospray y con el tiempo de retención y las intensidades, se obtiene la masa carga de toda la proteína (MS1). Posteriormente se fracciona en determinados puntos dentro de la proteína. Es más fácil obtener la estructura de la proteína. El MS2 permite ver la estructura de fragmentos de la proteína. 

En el caso del Bottom-Up, tenemos toda la proteína y se hace una digestión tríptica. Los péptidos se ionizan, y esos fragmentos ionizados son los que detectará el MS. En Top-Down, la proteína intacta se ioniza. Al fragmentar para el MS2, hay trozos y fragmentos de los iones con una conformación de la proteína más larga, siendo así más fácil ver la estructura completa. 

Comparando las dos técnicas en proteómica:
\begin{table}[h]
\centering
\begin{tabular}{p{4cm}|p{5cm}p{5cm}}
Aspecto & Top-Down & Bottom-Up \\ \hline
Tamaño de las proteínas & Adecuado para proteínas pequeñas y medianas & Adecuado para todo tamaño, especialmente proteínas grandes \\
Equipos & Instrumentos de alta resolución & Instrumentos más accesibles \\
Sensibilidad & Menor, difícil para proteínas en baja abundancia & Mayor sensibilidad \\
Complejidad computacional & Alta, espectros más difíciles de interpretar & Menor, espectros de péptidos más sencillos \\
Cobertura & Cobertura completa de la proteína & Cobertura parcial (depende de la digestión) \\
Manejo de mezclas complejas & Difícil para mezclas de muchas proteínas & Más fácil, adecuado para muestras complejas \\
Detección de PTMs & Precisa, localización directa en la proteína intacta & Puede perder algunas PTMs, pero identifica modificaciones en péptidos \\
Preparación de muestras & Más compleja, requiere proteínas intactas & Más sencilla, con digestión enzimática
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width = 0.9\textwidth]{figs/topdown-vs-bottomup.png}
\end{figure}

\chapter{Identificación de péptidos mediante espectrometría de masas}
Las máquinas actuales generan millones de espectros MS/MS en muy poco tiempo. No se pueden interpretar manualmente, siendo necesario usar herramientas computacionales. 
Las estrategias más utilizadas para identificar péptidos mediante MS/MS son:
\begin{itemize}
\item \textbf{Secuenciación \textit{de novo}}: Consiste en obtener la secuencia del péptido directamente a partir del espectro.
Se usa en casos donde el genoma del organismo no está (o sólo parcialmente) secuenciado. La
secuencia se infiere directamente del espectro sin ayuda de una base de datos de referencia.
También se usa para identificar o caracterizar modificaciones postraduccionales.

\item \textbf{Búsqueda contra bases de datos}: Se identifica el péptido en la base de datos a partir del espectro MS/MS. Para ello se hace una "correlación" entre el espectro MS/MS obtenido experimentalmente y los espectros teóricos generados a partir de las secuencias de los péptidos en la base de datos.

Los motores de búsqueda se encargan de asignar a cada espectro obtenido experimentalmente un péptido, que es el mejor candidato de una lista de los posibles péptidos en la base de datos, de acuerdo a cierta \textbf{puntuación}, que mide el \textbf{grado de similitud entre el espectro empírico y el teórico}.
A cada una de estas \textbf{parejas péptido-espectro} se les denomina \textbf{PSM} (Peptide-Spectrum Match, Asignación Péptido-Espectro). 

Denominaremos \textbf{puntuaciones simples} a aquellos algoritmos que generan una puntuación para cada PSM que es independiente de la puntuación asignada al resto de PSMs en el mismo experimento.
Los algoritmos que asignan una puntuación a cada PSM considerando el comportamiento del resto de las PSM generan puntuaciones que denominaremos \textbf{complejas}.
\end{itemize}

\section{Algoritmos de identificación de péptidos - Puntuaciones simples}
Hay varios algoritmos para poder obtener esta puntuación:
\begin{itemize}
\item \textbf{Algoritmos empíricos de puntuación}
\begin{itemize}
\item SEQUEST
\item Reginamiento de las puntuaciones SEQUEST
\end{itemize}
\item \textbf{Algoritmos basados en la probabilidad del emparejado}
\begin{itemize}
\item Mascot
\item Andromeda
\end{itemize}
\item \textbf{Algoritmos basados en la probabilidad del candidato entre el resto de los candidatos}
\begin{itemize}
\item p-value y e-value
\item X!Tandem
\item MSFragger
\end{itemize}
\end{itemize}

\subsection{Algoritmos empíricos de puntuación}
SEQUEST mide el grado de similitud entre el espectro adquirido y el espectro teórico. Todos los péptidos tienen una suma de masas que concuerdan con el espectro y un rango de posible error. Tras obtener la lista de los fragmentos que coinciden con la suma de masas, se crea un espectro teórico y se compara uno y otro. En caso de SEQUEST, es ver que los picos de fragmentos coincidan con ellos. El score da la suma de cada fragmento con un margen de error y ver si coincide. Con esto se selecciona el fragmento con mayor similitud y nos quedamos con él. 

A partir de esto, aparecen otras puntuaciones de refinamiento. Un ejemplo es $\Delta C_n$. Este algoritmo indica el valor de la diferencia entre las puntuaciones ordenadas. SI el segundo candidato en puntuación es muy bajo (hay una gran diferencia), nos podemos fiar más del primer candidato por el score obtenido. Puede suceder que el valor de $\Delta C_n$ sea alto, pero la puntuación base sea baja. O al revés, que haya muy poca diferencia entre puntuaciones, pero éstas sean muy altas. 

Otro refinamiento de SEQUEST es $cXcorr$. Éste refinamiento depende de la masa: a mayor masa, más aminoácidos, habiendo más fragmentos que emparejar y un valor más elevado de Xcorr. También depende de la carga, ya que a igual m/z, los péptidos con más carga tienen más aminoácidos, por lo que el valor de Xcorr también tiende a ser más alto. Por tanto, Xcorr da prioridad a los péptidos de mayor masa y de mayor carga. Una forma de corregir este efecto es con cXcorr, que se hace independiente de la longitud del péptido.

\subsection{Probabilidad en el emparejado}
\paragraph{Mascot}
Mascot puntúa el emparejamiento entre una secuencia y un espectro MS/MS calculando la probabilidad de que el emparejamiento sea un evento aleatorio. El algoritmo no se ha publicado. Utiliza la puntuación Ion Score (IS) que se define como $IS = -10 \cdot \log P$, donde $P$ es la probabilidad de que la coincidencia observada entre el espetro teórico y el experimental sea un evento aleatorio. El IS refleja la calidad del emparejamiento. Cuanto más bajo sea P, mayor será el puntaje; es decir, una puntuación alta es indicativo de un emparejamiento más fiable entre el espectro experimental y la secuencia teórica. Se trata de un software comercial de uso gratuito para búsquedas pequeñas.

\paragraph{Andromeda}
Andromeda es un motor de búsqueda de péptidos que utiliza un concepto semejante a Mascot. El algoritmo ha sido publicado. Se basa en el cálculo de la probabilidad binomial de tener un número de fragmentos en el espectro que coincidan al azar. Andromeda se ofrece como “software” libre que puede funcionar de forma independiente o integrada dentro del paquete MaxQuant.

%26/02 - José Manuel
\subsection{Probabilidad del candidato}
\subsubsection{p-value y función de densidad de probabilidad (FDP)}
Para calcular el p-valor (p-value) se tienen en cuenta las puntuaciones obtenidas por todos los candidatos de la base de datos contra los que se compara un determinado espectro MS/MS.
El p-value de una puntuación x se define como la probabilidad de que el espectro encuentre una PSM de puntuación igual o mejor que x. Esta probabilidad se puede obtener a partir de la Función de Densidad de Probabilidad (FDP o Probability Density Distribution). Para generar la FDP se ordenan las puntuaciones de menor a mayor (1), se construye un histograma y se ajusta a una función empírica (2), se normaliza la función para que el área sea la unidad (3) y se calcula el área por encima de la puntuación (s) dada. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.9\textwidth]{figs/fdp.png}
\end{figure}

\subsubsection{Valor esperado (e-value)}
Supongamos que obtenemos una PSM con p-value = 0.001 después de buscar contra N = 1000 secuencias candidatas. ¿Se puede considerar este resultado significativo?
La probabilidad de obtener una o más coincidencias con una probabilidad p o menor cuando el evento se repite N veces es igual a la probabilidad de no tener N coincidencias con (1 - p), es decir:
$$p(\text{one or more matches with p or lower}) = 1 - (1 - p)^N \sim Np$$

Como se indica en la fórmula, cuando p es un valor pequeño, esta probabilidad se aproxima al producto de N y p. En el caso comentado, Np = 1000x0.001 = 1 y, por lo tanto, la coincidencia no es significativa. En otras palabras, esperaríamos que al menos una secuencia puntuaría con p = 0.001 por puro azar.

El valor esperado (e-value o expectation value) se define como el producto de N por p, y mide el número esperado de coincidencias con un valor p que se obtendría cuando el espectro MS/MS se busca contra N candidatos.

$$e = N \cdot p$$

El e-value es una forma compacta de calcular la probabilidad real de forma aproximada, y da una idea más realista de la significatividad de una PSM.

\subsubsection{X!Tandem}
X!Tandem es un motor de búsqueda que usa una puntuación llamada hyperscore (HS).
$$HS = (\sum^n_{i = 0} I_i P_i) N_b!N_y!$$
La puntuación HS tiene en cuenta la intensidad de los fragmentos que coinciden entre el espectro observado y el teórico, y el número de iones “b” e iones “y” asignados. Además, X!Tandem calcula el p-value asociado al HS para determinar si una identificación es correcta o no.

\subsubsection{MSFragger}
MSFragger es un motor de búsqueda ultrarrápido que cuya puntuación y modelado probabilístico son muy parecidos a X!Tandem. La búsqueda ultrarrápida se consigue construyendo un theoretical fragment index a partir de todas las secuencias de las bases de datos. Cada fragmento en el espectro observado se compara con todos los fragmentos de la misma masa de todos los candidatos en un solo paso.

MSFragger es adecuado para cualquier análisis de proteómica de shotgun, para búsquedas sin restricciones de enzimas, búsquedas de bases de datos “abiertas”, y para la identificación de péptidos modificados glicopéptidos. MSFragger es suficientemente rápido para ser usado incluso en un ordenador portátil.

\subsubsection{Mejor método}
Se han publicado muchas comparativas entre los diferentes motores de búsqueda, pero la situación no está clara. Algunos algoritmos funcionan mejor para ciertos péptidos, mientras que otros funcionan mejor para diferentes conjuntos de péptidos. La principal preocupación no es cuál debería usarse, sino cómo interpretar correctamente los resultados. Por lo tanto, si nos encontramos cómodos con un método de identificación de péptidos MS/MS (conocido), lo mejor es seguir usándolo. Lo importante es asegurarse de que la \textbf{validación de las identificaciones} está hecha correctamente. 

\section{Algoritmos de identificación de péptidos - Puntuaciones complejas y validación estadística}
\subsection{Validación mediante Posterior Error Probability (PEP)}
\subsubsection{PeptideProphet - Función Discriminatoria}
PeptideProphet fue en su momento un nuevo concepto de algoritmo que tiene en cuenta todos los factores que pueden influir en un buscador a la hora de decidir si una identificación es correcta. Utiliza una \textbf{función discriminatoria F} que se construye como combinación lineal de todos los factores (Xcorr, $\Delta$Cn, Ln SpRank, etc.):
$$F(x_1, x_2, \ldots, x_s) = c_0 + c_1x_1 + c_2x_2 + c_3x_3 + \ldots + c_sx_s$$
Se aplicó originalmente a las puntuaciones de Sequest, pero el concepto es de aplicabilidad universal y se ha usado con otros motores de búsqueda (Mascot, Andromeda, X!Tandem, etc). 

La función F primero se aplica a un conjunto de datos de entrenamiento (una población de espectros MS/MS bien caracterizados), con los que se optimizan los coeficientes lineales que mejor discriminan entre identificaciones correctas e incorrectas mediante un algoritmo iterativo. De esta manera se obtienen distribuciones de F separadas para las identificaciones de péptidos correctas (positivas) e incorrectas (negativas).

\begin{figure}[h]
\centering
\includegraphics[width = 0.9\textwidth]{figs/peptideprophet.png}
\end{figure}

\subsubsection{Cálculo bayesiano de PEP}
Las funciones F se normalizan para convertirlas en funciones de densidad de probabilidad (PDF), de manera que la distribución de todos los péptidos se expresa como una superimposición de la de los ciertos y la de los falsos. A cada puntuación “s” se le asigna un valor PEP(s), que se define como la probabilidad de obtener un péptido falso (negativo ó N) cuando se obtiene esa puntuación “s” ("N dado s"), es decir la probabilidad bayesiana P(N/s). P(N/s) se calcula como el cociente entre la PDF de la distribución de negativos (f) y la de la distribución total (t).

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/bayesiano.png}
\end{figure}

\subsection{Validación mediante conceptos estadísticos globales}
\subsubsection{Tabla de contingencia}
Las tablas de contingencia se utilizan en estadística para valorar de qué manera un parámetro (en este caso la puntuación) permite discernir entre dos condiciones dadas (en este caso entre asignaciones verdaderas y falsas). En el caso ideal hay un umbral por encima del cual todas son verdaderas o todas son falsas. Idealmente, hay una separación perfecta, pero en la realidad, la separación entre ciertos y falsos es incompleta.

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/contingencia.png}
\end{figure}

\subsubsection{Tasa de Falsos Positivos (FPR, False Positive Rate)}
La tasa de falsos positivos es la proporción de asignaciones consideradas correctas entre el total de asignaciones incorrectas.
$$FPR = 1 - especificidad = \frac{FP}{TN + FP}$$

La FPR equivale al p-value de la distribución global de puntuaciones.

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/fpr.png}
\end{figure}

\subsubsection{Tasa de Falsos Descubrimientos (FDR, False Discovery Rate)}
La tasa de falsos descubrimientos es la proporción de asignaciones consideradas incorrectas entre el total de asignaciones consideradas (por encima del umbral).
$$FDR = \frac{FP}{TP + FP}$$

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/fdr.png}
\end{figure}

Muy importante: la FDR es el método idóneo para corregir la FPR (ó p-value) cuando se realizan test múltiples (cuando se identifican muchos péptidos al mismo tiempo en experimentos a gran escala).

\subsubsection{Errores tipo I y errores tipo II}
\begin{itemize}
\item \textbf{Error de tipo I}, es el error que se comete cuando el investigador rechaza la hipótesis nula siendo ésta verdadera. Se evalúa calculando la probabilidad de encontrar un resultado falso positivo.
\item \textbf{Error de tipo II}, es el error que se comete cuando no se rechaza la hipótesis nula siendo falsa. Se evalúa calculando la probabilidad de encontrar un resultado falso negativo.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/error-types.png}
\end{figure}

\subsection{Estrategia Target-Decoy para el cálculo de la FDR}
\subsubsection{Uso de bases de datos señuelo o decoy}
Este es un método muy robusto y ampliamente aceptado. Se busca sobre las bases de datos de objetivo y señuelo. Se utilizan los resultados de la base de datos señuelo para estimar el número de falsos positivos en un umbral de puntaje dado.

La base de datos señuelo ("decoy") es una base de datos idéntica a la objetivo ("target") pero invirtiendo el orden de las secuencias de aminoácidos de cada proteína (C-terminal se convierte en N-terminal y viceversa), o invirtiendo las secuencias de aminoácidos de cada péptido, manteniendo los extremos básicos N-terminales (pseudoreversión).

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/decoydb.png}
\end{figure}

Las bases de datos aleatorias no se comportan de manera idéntica y sobreestiman las tasas de error (la repetición de dominios y/o motivos en la base de datos de objetivo reduce el número total de secuencias únicas).

\subsubsection{Bases de datos separadas vs bases de datos concatenadas}
En las \textbf{bases de datos separadas}, la búsqueda se realiza contra las bases de datos objetivo y señuelo de forma separada. Para cada puntuación se calcula:
\begin{itemize}
\item D: número de péptidos identificados en la base de datos señuelo (falsos)
\item T: número de péptidos identificados en la base de datos objetivo (total = ciertos + falsos)
\end{itemize}
La FDR se calcula de la siguiente manera: $FDR = D / T$.

En las \textbf{bases de datos concatenadas}, la búsqueda se realiza contra una base de datos compuesta por la base de datos señuelo, unida a la base de datos objetivo. En este caso, el número de falsos identificados en cada puntuación equivale al doble de D, y el total equivale a D + T, por lo que $FDR = 2 * D / (D + T)$

El primer método tiende a sobreestimar el número de falsos positivos, porque los espectros MS/MS de alta calidad (que corresponde a asignaciones correctas) tienes a dar puntuaciones altas en la base de datos. El segundo método evita este efecto al permitir que las secuencias objetivo y señuelo compitan por los mejores espectros, pero la FDR se calcula en una población "inflada" (contiene D). En la actualidad se prefiere el uso de bases de datos concatenadas y la FDR se calcula como $FDR = D / T$.

\subsubsection{Uso de Decoys en el modelo bayesiano: adaptación de PeptideProphet}
El enfoque de modelado probabilístico de PeptideProphet y la estrategia target-decoy se pueden combinar dentro de un solo marco semi-supervisado. Se utiliza un algoritmo de maximización de expectativas (EM) semi-supervisado para construir un clasificador bayesiano para la identificación de péptidos usando como falsos las puntuaciones “decoy” y como ciertos las puntuaciones “target”. Ello permite calcular la FDR además de la PEP.

\subsection{Percolator y Aprendizaje Automático}
Percolator utiliza un método de aprendizaje semi-supervisado automático iterativo, que se entrena para optimizar la separación entre asignaciones “target” y “decoy”.
Conceptualmente equivale a Peptide-Prophet (con la estrategia target-decoy), pero en vez de usar una función discriminatoria incluye los parámetros de la búsqueda utilizando algoritmos tipo machine-learning.
Entrena un algoritmo de aprendizaje automático, Máquinas de vectores de soporte (SVM), para discriminar entre PSM positivas y negativas.
Percolator calcula directamente la FDR usando la estrategia target-decoy.

\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{figs/percolator-pseudocode.png}
\end{figure}

\subsection{Comparativa entre PEP y FDR}
PEP y FDR son parámetros que se basan en la misma distribución de ciertos (target) y falsos (decoy). PEP mide la proporción “local” de falsos en la puntuación “s” (valor en la curva). FDR mide la proporción de falsos que tienen una puntuación igual o mayor que “s” (área de la curva). Por ello PEP se puede interpretar como la FDR “local” en la puntuación “s”. PEP es más adecuada para valorar la identificación de una PSM aislada. FDR es la que debe usarse en experimentos a gran escala cuando queremos identificar muchas PSM al mismo tiempo.
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/pep-fdr.png}
\end{figure}

%05/03 - José Manuel
\chapter{Cuantificación de proteínas}
La cuantificación de proteínas consiste en cuantificar en distintas muestras o situaciones (caso-control, a distintos tiempos, etc). Las mediciones que hay en espectometría de masas se distinguen en dos enfoques: la cuantificación sin marcaje (label-free) y con marcaje. Dentro de las cuantificaciones label-free hay una relacionada con la cuantificación de los espectros MS2 o mediante picos. Las muestras de cada caso se analizan por separado. En el caso del marcaje, al tener más posibles muestras a analizar, se analizan todas de forma conjunta. De esta forma, se reduce el error entre diferentes tandas, que es el error sistemático del propio espectrómetro. 

Las diferencias entre marcaje y label-free son las siguientes:
\begin{table}[h]
\centering
\begin{tabular}{p{2.5cm}|p{6cm}p{6cm}}
Características & Label-Free & Marcaje \\ \hline
Uso de etiquetas & No utiliza etiquetas ni marcadores químicos & Utiliza etiquetas químicas o isotópicas. \\
Multiplexación & No permite multiplexación directa; se analizan las muestras por separado & Permite analizar múltiples muestras simultáneamente mediante etiquetas únicas. \\
Sensibilidad y precisión & Puede ser menos sensible y preciso, especialmente en muestras complejas & Mayor sensibilidad y precisión en la cuantificación \\
Flexibilidad & Mayor flexibilidad, no requiere preparación previa con etiquetas & Menor flexibilidad debido al proceso de marcado. \\
Tiempo de procesamiento & Más rápido, ya que no incluye pasos de marcado & Más lento debido a la preparación y marcado de muestras \\
Costo & Más económico, ya que no requiere reactivos de marcaje & Más costoso debido al uso de reactivos específicos para el marcaje. \\
Aplicaciones principales & Estudios exploratorios, análisis de grandes cohortes, biomarcadores emergentes & Análisis comparativo detallado, estudios cuantitativos precisos. \\
Complejidad del análisis & Más simple en términos de preparación de muestras & Más complejo debido a la necesidad de etiquetar y calibrar. \\
Ejemplos de técnicas & Espectrometría de masas label-free (DDA, DIA) & SILAC, iTRAQ, TMT.
\end{tabular}
\end{table}

\section{Cuantificación con label-free}
Las muestras pasan por el espectrómetro de forma separada. Hay dos MS1 por cada muestra (caso y control) con su tiempo de retención, intensidad y masa por carga. A la hora de obtener la cuantificación, se cuenta cada uno de los espectros de fragmentación. Por la cuantifiación de espectros se puede llegar a saber la cantidad de proteína que hay para las distintas muestras. Estos espectros identifican luego a un péptido, y el péptido a una proteína. 

Para una cuantificación por integración de picos, se tienen en cuenta el área bajo la curva del pico de fragmentación del MS1. Ese área con respecto a la intensidad da lugar a la cuantificación del péptido en cuestión.
Para una proteína, se suman las intensidades de los péptidos que definen a esa proteína.

\subsection{Estrategia Match-Between-Runs}
En función de la dilución, hay péptidos que pueden no ser identificados. La estrategia MBR asigna péptidos no identificados en una muestra desde otra muestra. Se ordenan los péptidos en los tiempos de retención (donde se espera que el péptido sea identificado). Esto se hace para todas las muestras. Luego se ajustan los tiempos de retención entre las muestras. De esta forma, se identifican péptidos en una muestra que falta en otra, migrando así los datos. Esta estrategia hay que cogerla con pinzas al coger falsos positivos de una muestra a otra. Por ello, solo se recomienda hacer en casos con muestras muy parecidas o donde deben aparecer las mismas proteínas. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{figs/mbr.png}
\end{figure}

\section{Cuantificación con marcaje}
\subsection{Cuantificación con SILAC}
Este método está pensado para células. Tenemos los cultivos de células, y cada célula se marca con isótopos más pesados. Así, se incrementa la masa carga de las proteínas. Se mezclan los cultivos y se digiere para generar los péptidos. Al tener las muestras con y sin marcaje juntas, los espectros se diferenciarán en la masa carga. Se ve la abundancia entre cada uno de los péptidos encontrados, comparando la intensidad asociada a cada señal. 

\subsection{Cuantificación con marcaje iTRAQ/TMT}
Con iTRAQ se pueden tener hasta 4 muestras debido a los reactivos a utilizar, mientras que para TMT podemos tener hasta 18 muestras. Los reactivos son específicos para ellos. La particularidad de este marcaje consiste en digerir los péptidos y añadir unos reactivos específicos para cada uno de los canales o de las muestras. Lo novedoso es que la composición química de los reactivos hace que los péptidos tenga la misma masa para cada una de las muestras, apareciendo así con un único pico de intensidad para MS1. El pico más alto (el péptido más intenso) se vuelve a fragmentar para generar el MS2. En el MS/MS hay una región reportera que representa la cantidad del péptido en cada muestra. Para la cuantificación del péptido se mide la intensidad del ion del marcaje. Como los distintos marcajes pesan lo mismo en los péptidos o proteínas, se pueden comparar directamente los picos de intensidad al representar la diferencia en la cuantificación de las distintas muestras. 

\subsection{Cuantificación con péptidos sintéticos}
Los péptidos sintéticos se pueden utilizar como estándar interno para comparar la cuantificación de las distintas muestras. 

\chapter{Identificación de proteínas mediante espectrometría de masas}
Las proteínas se identifican a partir de sus péptidos. Una proteína puede ser identificada por uno o más péptidos. A la hora de valorar las identificaciones a nivel de proteína surgen diversos problemas para los que se han propuesto varias soluciones.
\begin{itemize}
\item \textbf{Redundancia peptídica:} grupos de proteínas.
\item \textbf{Cálculo de la FDR a nivel de proteína usando la estrategia target-decoy:} el cálculo de la FDR de proteína se puede calcular a partir de sus péptidos o a partir de la puntuación de la proteína.
\end{itemize}

\section{Redundancia peptídica}
Los espectros MS/MS identifican un péptido, pero la secuencia del péptido puede pertenecer a más de una proteína posible. Se dice que los péptidos compartidos entre varias proteínas están "degenerados", y presentan un desafío ya que no puede saberse cuál de las proteínas a las que pertenece está siendo identificada. Esto suele ocurrir con mayor frecuencia en proteínas homólogas, variantes de empalme (alternative splicing) o debido a entradas redundantes en la base de datos de secuencias de proteínas. Los péptidos redundantes son más frecuentes en las búsquedas que utilizan bases de datos grandes.

Los principales enfoques para solucionar la redundancia peptídica son:
\begin{itemize}
\item \textbf{Péptidos proteotípicos:} se basa en usar solo péptidos cuya secuencia sea exclusiva de una única proteína.
\item \textbf{Occam's razor}: se basa en el principio de la "navaja de Occam", que sostiene que la mejor explicación de un fenómeno es la explicación más sencilla. Se busca la combinación mínima de proteínas que explique los péptidos observados.
\item \textbf{Grupos proteicos}: En vez de identificar proteínas individuales se identifican "grupos de proteínas" que comparten péptidos, evitando así la eliminación de proteínas válidas pero sin inflar el recuento de proteínas. Existen varias herramientas (PeptideClassifier, IsoformResolver, PAnalyzer) que difieren en la forma de clasificar y agrupar.
\item \textbf{Modelos de probabilidad a nivel de proteína (ProteinProphet)}: El modelo calcula la probabilidad de cada proteína a partir de las PEP de sus péptidos. Los péptidos se reparten entre las proteínas de acuerdo a la probabilidad de cada una, y se vuelve a calcular la probabilidad de cada proteína. El algoritmo itera hasta que los péptidos quedan "repartidos" entre las proteínas por un factor de peso. Por tanto este modelo asume que cada péptido puede pertenecer a más de una proteína.
\end{itemize}

\subsection{Agrupación de péptidos en proteínas: Grupos de proteínas}
De acuerdo a los péptidos compartidos, las proteínas pueden clasificarse de muchas maneras. Las diferentes entradas de la base de datos de secuencias podrían contarse como identificaciones de proteínas separadas cuando, de hecho, todas comparten el mismo conjunto de péptidos y, por lo tanto, son indistinguibles. En la mayoría de los casos, estas redundancias no solo inflan el número total de proteínas informadas como identificadas, sino que también pueden conducir a una interpretación biológica incorrecta de los datos.

El problema se complica aún más cuando no se realiza un análisis estadístico para determinar la validez de las identificaciones de péptidos y proteínas. Por lo tanto, existe la necesidad de desarrollar una nomenclatura común y un conjunto de pautas para asignar péptidos a proteínas y para interpretar los conjuntos de datos de identificación de proteínas resultantes.

\subsection{Agrupamiento de proteínas en PAnalyzer}
Considera tres tipos de péptidos:
\begin{itemize}
\item \textbf{Únicos:} asignables a una única proteína
\item \textbf{Discriminantes:} compartidos por proteínas sin péptidos únicos
\item \textbf{No discriminantes:} compartidos por proteínas con péptidos únicos o discriminantes
\end{itemize}

En función de estos péptidos agrupa las proteínas según cuatro tipos de evidencia:
\begin{itemize}
\item \textbf{Concluyentes:} proteínas con algún péptido único.
\item \textbf{Indistinguibles:} proteínas con péptidos discriminantes sólo compartidos por ellas y que también comparten el resto de péptidos.
\item \textbf{Grupo ambiguo:} proteínas que comparten péptidos discriminantes (y no son indistinguibles).
\item \textbf{No concluyentes:} proteína que sólo tiene péptidos no discriminantes.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/panalyzer.png}
\end{figure}

Las \textbf{proteínas indistinguibles} mayoritariamente se corresponden con isoformas y su grupo con el gen correspondiente. En este caso se podría dar evidencia concluyente a nivel de gen pero no a nivel de isoforma. En los \textbf{grupos ambiguos} algunas proteínas tienen que estar presentes, pero no sabemos cuáles. La explicación más sencilla (Occam) no tiene por qué ser la real. Las \textbf{proteínas no concluyentes} no tienen por qué estar presentes para explicar los péptidos observados, no deberían "contar".

El algoritmo empleado por PAnalyzer comienza marcando los péptidos únicos y sus correspondientes proteínas como concluyentes. Los péptidos compartidos presentes en proteínas concluyentes serán no discriminantes. El resto de péptidos se recorren formando grupos de proteínas temporales para decidir cuáles son discriminantes y cuáles no. Una vez marcados todos los péptidos se forman los grupos de proteínas definitivos.

\section{Cálculo de la FDR de proteína a partir de sus péptidos}
\subsection{Cálculo de la FDR a nivel de proteína usando la estrategia target-decoy}
Igual que los péptidos, la identificación de proteínas puede validarse calculando la FDR a nivel de proteína a partir de la estrategia target-decoy. Cada proteína puede identificarse mediante varios péptidos. La estrategia más sencilla para calcular la FDR a nivel de proteína consiste en llevar a cabo la identificación a nivel de péptido:
\begin{itemize}
\item Se calcula la FDR a nivel de péptido usando la estrategia target-decoy
\item Se seleccionan los péptidos target identificados correctamente a la FDR considerada
\item Se seleccionan también los péptidos decoy por encima del umbral de puntuación
\item Los péptidos target se usan para identificar proteínas target
\item Los péptidos decoy se usan para identificar proteínas decoy
\item Se calcula la FDR a nivel de proteína de la misma manera que a nivel de péptido
\end{itemize}

$$FDRprot = \frac{\text{Número de proteínas Decoy}}{\text{Número de proteínas Target}}$$

Esta estrategia tiene el problema del hinchamiento de la FDR a nivel de proteína (aún así sigue siendo muy usada hoy en día), que es filtrar los péptidos target y decoy usando la FDR a nivel de péptido, a partir de ellos identificar proteínas target y decoy y a partir de ellas calcular la FDR a nivel de proteína.

\subsection{Hinchamiento de la FDR a nivel de proteína}
Las identificaciones de péptidos target tienden a concentrarse en las proteínas presentes en la muestra (proteínas reales), mientras que las identificaciones de péptidos decoy se distribuyen aleatoriamente entre las proteínas de la base de datos. Por tanto, la proporción entre el número de péptidos y el número de proteínas es mayor para los péptidos "Target" que para los péptidos "Decoy". Por lo tanto, la FDR a nivel de proteína es siempre mayor que a nivel de péptido. El problema se agudiza en experimentos donde se identifican números altos de proteínas. Este problema se denomina hinchamiento de la FDR a nivel de proteína (protein FDR buildup problem). Para resolver este problema existen varias soluciones:
\begin{itemize}
\item Bajar la FDR a nivel de péptido hasta conseguir una FDR aceptable a nivel de proteína. Este método es muy fiable, aunque disminuye mucho la sensibilidad de identificación. Se utiliza en experimentos donde la fiabilidad de la identificación es muy importante
\item La estrategia MAYU
\item Utilizar puntuaciones a nivel de proteína
\end{itemize}

%07/03
\section{Modificaciones posttraduccionales}
Actualmente se estima que haya 20.000 genes codificantes y 90.000 tránscritos codificantes. Hay distintos tipos de modificaciones que se pueden dar dentro de la célula: acetilaciones y metilaciones en las histonas, cascadas de fosforilación, etc. Por ejemplo:
\begin{itemize}
\item La activación de enzimas inactivas, como el tripsinógeno, que se convierte en tripsina activa mediante el corte de un fragmento específico.
\item Algunas PTM (como la ubiquitinación) marcan la proteína para su degradación por el proteosoma
\item Modificaciones (fosforilación o glicosilación) actúa como una señal para que otras moléculas reconozcan y se unan a la proteína.
\item Las histonas pueden ser acetiladas y metiladas en diferentes combinaciones, lo que regula la accesibilidad del ADN y la expresión génica
\end{itemize}

Las modificaciones posttraduccionales pueden ser reversibles (adición de grupos químicos, polipéptidos o moléculas complejas) o irreversibles (modificación de algún aminoácido o cortes dentro de la proteína).

\subsection{Modificaciones posttraduccionales más comunes}
\paragraph{Fosforilación: adición y eliminación de grupos fosfato}
La fosforilación proteica reversible controla gran parte de la actividad del proteoma celular. Esta modificación regula procesos clave como la diferenciación celular, el ciclo celular, el metabolismo y la apoptosis. También juega un papel esencial en la comunicación intracelular y la respuesta inmunitaria. La fosforilación puede activar o inactivar enzimas, reclutar otras proteínas y provocar cambios estructurales o de localización en las proteínas.

\paragraph{Glicosilación: adición de fracciones de azúcar}
Se unen moléculas de azúcar a las proteínas, formando glicoproteínas. Glicosilación ligada a N, se une en residuos de asparagina (N). Glicosilación ligada a O, se une en residuos de serina (S), treonina (T) o hidroxilisina (Hyl). Modificación crucial para el plegamiento, estabilidad e interacciones celulares de las proteínas. Participa en el control de calidad de las proteínas, protegiendo de la degradación. Implicada en la respuesta inmunitaria y la adhesión celular.

\paragraph{Ubiquitinación: selección de proteínas para su degradación}
La ubiquitinación, PTM que consiste en la unión covalente de ubiquitina a proteínas diana. Modificación que marca las proteínas para su degradación por el proteasoma, facilitando el recambio y control de calidad de las proteínas. Regula la homeostasis celular, la reparación del ADN, el ciclo celular y las respuestas inmunitarias. Ocurre mediante una reacción enzimática que involucra enzimas E1 (activadoras), E2 (conjugadoras) y E3 (ligasas).

\paragraph{N-acetilación: modificación en el extremo N}
Ocurre principalmente en residuos de lisina o en el extremo N de las proteínas. Regula la estabilidad de las proteínas, incluyendo su degradación (señal Ac/N-degron). Se asociaba con modificaciones epigenéticas en las histonas y también en proteínas no histónicas bajo control metabólico. Regula la función celular y puede combinarse con otras PTM para generar variantes funcionales. Desempeña un papel fundamental en la actividad y regulación de las proteínas.

\paragraph{Metilación: adición de grupos metilo}
Modificación que añade grupos metilo a residuos de lisina y arginina. Puede producirse como mono-, di- o trimetilación, lo que afecta diversas funciones de las proteínas. Papel clave en la regulación epigenética, influyendo en la estructura de la cromatina, la expresión génica y la reparación del ADN. También regula las interacciones proteína-proteína y puede estabilizar las proteínas al impedir su degradación.

\subsection{Técnicas de detección de modificaciones posttraduccionales}
Para la detección de las modificaciones posttraduccionales hay distintas técnicas: La \textbf{espectrometría de masas} caracteriza PTM con alta precisión, usando enfoques de análisis de proteínas. Técnicas basadas en \textbf{anticuerpos} (western blot e inmunoprecipitación) detectan y aíslan proteínas modificadas. Los \textbf{ensayos funcionales} evalúan cómo las PTM afectan la actividad de las proteínas. \textbf{Tecnologías emergentes} como BioID y APEX estudian interacciones proteína-proteína en células vivas.

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{figs/ms-ptm.png}
\end{figure}

\chapter{Metabolómica}
\section{Introducción a la metabolómica}
Los metabolitos son moléculas pequeñas presentes en fluidos biológicos, compartimentos celulares o tejidos. Se dividen en cinco categorías:
\begin{enumerate}
\item  Compuestos endógenos reales: Originados por reacciones enzimáticas bajo el control del genoma del huésped.
\item Factores esenciales transformados por el metabolismo: Como aminoácidos esenciales, vitaminas y cofactores.
\item Metabolitos derivados del co-metabolismo: Productos de la interacción entre el huésped y microorganismos (por ejemplo, la modificación de ácidos biliares por la microbiota intestinal).
\item Moléculas externas convertidas en compuestos endógenos.
\item Xenobióticos: Sustancias externas que influyen en el metabolismo del huésped, ya sea como productos o mediante interferencia.
\end{enumerate}

La metabolómica es el estudio del conjunto completo de metabolitos presentes en una célula o tejido bajo un conjunto específico de condiciones (conocido como metaboloma), generando un perfil bioquímico. Este perfil describe la bioquímica que define diferentes estados fisiológicos, incluyendo estados patológicos, que una célula o tejido puede adoptar.

Los objetivos de la metabolómica son identificar, caracterizar y cuantificar todas las moléculas biológicas que están involucradas en la estructura, función y dinámica de una célula, tejido u organismo.
\begin{enumerate}
\item Detección de cambios en el perfil metabólico de una enfermedad o condición
\item Comprender le mecanismo patológico/fisiológico en la base de una condición particular
\item Nueva estratificación de pacientes
\item Evaluar nuevas respuestas al tratamiento
\item Identificación de biomarcadores potenciales
\item Desarrollo de nuevas herramientas pronósticas y diagnósticas
\end{enumerate}

Hay tres estrategias fundamentales:
\begin{itemize}
\item Targeted analysis: tenemos un solo metabolito de interés
\item Profiling analysis: nos centramos en metabolitos específicos según su estructura química y/o actividad biológica
\item Un-targeted: sin sesgo, cribado global por clasificación de muestra
\end{itemize}

\section{Flujo de trabajo}
\subsection{Diseño experimental}
Buen conjunto de muestras correctamente clasificadas para evitar resultados impulsados por la variabilidad. Los grupos deben ser representativos del problema en estudio. Estudio en humanos: los participantes deben coincidir en edad, sexo, comorbilidades y IMC (BMI); los controles y la enfermedad deben definirse mediante pruebas de diagnóstico adecuadas. Estudio en animales o in vitro: modelo biológico representativo del problema en investigación. El diseño debe ser adecuado de acuerdo con las técnicas y el equipo disponibles. Aprobación del comité de ética. El muestreo debe ser realizado directamente por personal especializado utilizando protocolos estandarizados.

\subsection{Colección de muestras}
Los \textbf{tejidos} contienen información relacionada con alteraciones específicas \textit{in situ}: los cambios bioquímicos y la regulación upstream aparecen primero en el tejido y luego en los biofluidos.
Los \textbf{fluidos no endógenos} se obtienen mediante diversos procedimientos y son más adecuados para revelar mecanismos celulares cuando no se pueden utilizar tejidos.
Los \textbf{biofluidos} reflejan tanto cambios sistémicos como localizados.
Las \textbf{muestras \textit{in vitro}} pueden reproducir alteraciones \textit{in situ}.

\subsection{Tratamiento de muestras}
\begin{enumerate}
\item  \textbf{Descongelar la muestra en un baño de agua y hielo}. Esto es crucial para preservar la integridad de los metabolitos y evitar cambios químicos durante el proceso.
\item \textbf{Detención rápida de reacciones y extracción}: La muestra se somete a un proceso de "quenching" para detener rápidamente cualquier actividad metabólica o enzimática y, luego, se realiza la extracción de los metabolitos presentes.
\item \textbf{Secado}: Los componentes extraídos se someten a un proceso de secado (liofilización o deshidratación), para eliminar cualquier rastro de solvente y conservar los metabolitos en un estado seco.
\item \textbf{Re-suspender en un solvente adecuado}: Finalmente, la muestra seca se re-suspende en un solvente adecuado para su análisis posterior.
\end{enumerate}

\subsection{Análisis de las muestras}
Las muestras se pueden analizar por espectrometría de masas, al igual que en proteómica. 

\subsection{Tratamiento de datos}
Toda la información obtenida de los datos es difícil de cuantificar por Missing Values, mucho ruido y muchas variables. Los datos se preprocesan para ajustar determinados picos de masas por si ha habido pequeñas descalibraciones entre muestras.
Transformar los datos sin procesar en una matriz que contenga la lista de las características observadas con sus áreas de pico o intensidades relativas para cada muestra en un experimento determinado.
Los metabolitos se extraen y se agrupan caracterizaciones con las mismas ionizaciones y masa para reducir las caracterizaciones de los metabolitos.

A partir de los datos crudos se limpia la matriz:
\begin{itemize}
\item Filtrar por frecuencia: reducir el conjunto de datos en función de la diferencia biológica dentro de las muestras
\item Filtrar por variabilidad: eliminar resultados irrelevantes y no reproducibles, cuando la variabilidad analítica es mayor que la variación biológica.
\item Reemplazo de valores ausentes: los datos no disponibles para el análisis se pueden deber por una señal no presente o por debajo del límite de detección o por un error de software durante la extracción de características.
\item Corregir la heterocedasticidad reduciendo el impacto de posibles valores atípicos y convirtiendo los datos en una distribución más normal tipo Gaussiana.
\item Normalización para minimizar el error sistemático en los datos debido a la variación en las concentraciones de metabolitos, la sensibilidad y el rendimiento del instrumento, haciendo que los espectros sean comparables entre diferentes análisis.
\item Escalado: importancia de ajustar las variables del modelo dividiendo cada una de ellas por un factor de escala diferente.
\end{itemize}

\subsection{Análisis estadístico}
Se clasifican las muestras para establecer correlaciones entre variables, identificar variables significativas y ayudar a evaluar la calidad de todo el experimento. En el univariate data analysis se prueba la significancia estadística de compuestos individuales entre diferentes grupos de muestras. Para el multivariate data analysis, son estrategias útiles para modelar datos complejos para reducir y resumir la gran cantidad de variables en unas pocas variables latentes que expliquen la correlación existente simultáneamente entre ellas.

\subsection{Identificación de metabolitos}
No existe un código genético subyacente del cuals e pueda deducir la composición química.
Solo cuando tenemos metabolitos rigurosamente identificados, podemos involucrarnos completamente en la descripción de las vías metabólicas e integrar el metabolismo con otros niveles de la jerarquía biológica

\subsection{Hipótesis}
Los metabolitos son el resultado de procesos bioquímicos, como las cascadas de señalización. A través de ellos podemos reconocer vías alteradas y obtener una visión general de todos los cambios metabólicos que ocurren en una enfermedad. Pueden estar conectados con proteínas, transcritos y datos ambientales, lo que lleva a la revelación de biomarcadores en diferentes niveles.

\subsection{Validación}
La validación es fundamental para probar los hallazgos estadísticos. La efectividad de los biomarcadores candidatos como herramientas diagnósticas o pronósticas debe ser evaluada en una población amplia. Se requieren estudios clínicos para confirmar el posible uso de estas moléculas en un procedimiento rutinario. El mecanismo potencial debe ser confirmado en ratones knockout, donde las vías involucradas son estimuladas a propósito o inhibidas por modificaciones genéticas.

Entre los desafíos se encuentran las inconsistencias en los biomarcadores identificados entre experimentos similares, la necesidad de equipos interdisciplinarios y un biobanco de gran población.

\section{Metabolómica dirigida}
Para la metabolómica dirigida, se extraen los metabolitos con protocolos adaptados según la naturaleza del biomarcador candidato. Los métodos dirigidos de LC-MSMS son específicos, sensibles y rápidos para analizar un gran número de muestras. En cuanto a la cuantificación, se utilizan MS para obtener la concentración de metabolitos. Se utiliza un estándar interno marcado con isótopos estables, compuestos donde varios átomos del analito son reemplazados por sus respectivos isótopos estables. Al tener las mismas o similares propiedades físicas y químicas que el analito, pueden añadirse con una concentración conocida a las muestras antes de la extracción de metabolitos. 